{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Build Baseline Location Classifier\\nApproaches:\\n- instance representation in form of 'bag' of words \\n    - features: word frequencies, metadata \\n    - exclude rare words in data set (used by less than 3 users )\\n- gramatical structure with NLP\\n- model instances in terms of authors instead of documents \\n\\n\\n- Baseline Classifier: Naive Bayes Model ()\\n\\n\""
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Build Baseline Location Classifier\n",
    "Approaches:\n",
    "- instance representation in form of 'bag' of words \n",
    "    - features: word frequencies, metadata \n",
    "    - exclude rare words in data set (used by less than 3 users )\n",
    "- gramatical structure with NLP\n",
    "- model instances in terms of authors instead of documents \n",
    "\n",
    "\n",
    "- Baseline Classifier: Naive Bayes Model ()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from os import path, getcwd\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re, string\n",
    "import validators\n",
    "from info_gain.info_gain import info_gain_ratio\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# nltk.download('english')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Instance_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>\\ud83c\\udf17 @ Melbourne, Victoria, Australia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>@theage Of course it costs more, minimum stand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Hope people make just as much noise as they di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Perth</td>\n",
       "      <td>Pouring the perfect Prosecco \\ud83e\\udd42\\ud83...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Perth</td>\n",
       "      <td>$LNY losing traction at 0.014, see this retrac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>\\u0e44\\u0e21\\u0e48\\u0e44\\u0e2b\\u0e27\\u0e41\\u0e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>@ashleighjayy_ Me @ this bitch https://t.co/8J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>@AnaOLFan I \\u2764\\ufe0f you - I could never b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sydney</td>\n",
       "      <td>@ihatejoelkim Welcome to Australia! Hoping you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Mea evolve conference @markbouris session. I L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>First taste of Winter in Autumn https://t.co/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>\\u0643\\u064a\\u0641 \\u064a\\u0634\\u0648\\u0641 \\u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Sydney</td>\n",
       "      <td>#SelfieSunday #Lost #Tabby &amp;amp; White/Caramel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>@artsdesire Give her credit, she looks incredi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Sydney</td>\n",
       "      <td>@Pringster78 @Chadderbox2018 I kind of do the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Perth</td>\n",
       "      <td>@gramercypark It\\u2019s great isn\\u2019t it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sydney</td>\n",
       "      <td>https://t.co/RT37Cj5KRF  I date her, not MJ xo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Perth</td>\n",
       "      <td>@AussieAndyCx @ImScaredq Deadest Kane would de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Sydney</td>\n",
       "      <td>if i see got spoilers (and esp not tagged or a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>\\u0639\\u064e\\u0644\\u064a\\u0643 \\u0639\\u0648\\u0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Perth</td>\n",
       "      <td>@pleaseuseaussie @RodS108443078 Hope so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>x2 babe \\ud83d\\ude2b biggest scam I tell you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>@idgimaddy @EXOGlobal @B_hundred_Hyun @1992050...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Perth</td>\n",
       "      <td>sbren sbeve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Sydney</td>\n",
       "      <td>Grassy, bitter (Kiwi) hops, clean - Drinking a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>@FatherBob  Father Bob,  Thankyou I went to an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>30th Birthday Dinner \\ud83e\\udd42\\ud83c\\udf88\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Perth</td>\n",
       "      <td>@clementine_ford Us fellas were all drawn to i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Perth</td>\n",
       "      <td>https://t.co/nejALF6216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>KOKO BUNNY! The Easter Bunny arrived at my doo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103335</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Wow, I\\u2019m not very good at getting the pic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103336</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>I gather the Govt wants to further clog up Pun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103337</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Visualization is the key to success. Dream big...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103338</th>\n",
       "      <td>Sydney</td>\n",
       "      <td>Countries I\\u2019ve Been To:  Fiji \\ud83c\\udde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103339</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>@TaodeHaas @carolemorrissey Yes i agree....   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103340</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>@Wiley_Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103341</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Unbelievable @RiflePete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103342</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Thanks @TheJosephNaim for coming to see @Jerse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103343</th>\n",
       "      <td>Sydney</td>\n",
       "      <td>@HarryTrevor8888 @blondebonnie94 It is incompa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103344</th>\n",
       "      <td>Perth</td>\n",
       "      <td>The day before the budget, Sportsbet had odds ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103345</th>\n",
       "      <td>Perth</td>\n",
       "      <td>@sophie_walsh9 Where was this Sophie ? He was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103346</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>People who are not scientists but who question...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103347</th>\n",
       "      <td>Sydney</td>\n",
       "      <td>My mom came up to visit this morning and to ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103348</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>TS7 \\u2764\\ufe0f https://t.co/OQOaAft5Fg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103349</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>@iamtheoracle I know, I know. Those people are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103350</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>@StephanieLoveUK Already seen enough there\\u20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103351</th>\n",
       "      <td>Perth</td>\n",
       "      <td>@colinhussey22 gorillaz\\ud83d\\udc4c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103352</th>\n",
       "      <td>Sydney</td>\n",
       "      <td>@insufferablscot @tinyelbows_ Part of me think...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103353</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>@CDeLaFuentez Lol exactly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103354</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>@rmharmon2004 outside of the context ofwrestli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103355</th>\n",
       "      <td>Sydney</td>\n",
       "      <td>@KirstyWebeck So SMASH it Kirsty!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103356</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>\\u0e04\\u0e34\\u0e14\\u0e16\\u0e36\\u0e07\\u0e2b\\u0e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103357</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>@tbdalton5 That they are! I have had 2 in my l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103358</th>\n",
       "      <td>Perth</td>\n",
       "      <td>@GrayConnolly @GemmaTognini @ElizaJBarr You\\u2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103359</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>@australian @IzzyFolau very interesting that y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103360</th>\n",
       "      <td>Sydney</td>\n",
       "      <td>Solzinho bom pra ir na praia hoje</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103361</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>@suzdavies13 Of me??? Thank you, surely you do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103362</th>\n",
       "      <td>Perth</td>\n",
       "      <td>@GraceSpelman have you heard of Dune? seen som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103363</th>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Id bang you harder than an old Chevrolet door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103364</th>\n",
       "      <td>Sydney</td>\n",
       "      <td>@Mezmfield @chief_comanche @Utopiana @fraser_a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103360 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Location                                               Text\n",
       "Instance_ID                                                              \n",
       "1            Melbourne  \\ud83c\\udf17 @ Melbourne, Victoria, Australia ...\n",
       "2            Melbourne  @theage Of course it costs more, minimum stand...\n",
       "3             Brisbane  Hope people make just as much noise as they di...\n",
       "4                Perth  Pouring the perfect Prosecco \\ud83e\\udd42\\ud83...\n",
       "5                Perth  $LNY losing traction at 0.014, see this retrac...\n",
       "6            Melbourne  \\u0e44\\u0e21\\u0e48\\u0e44\\u0e2b\\u0e27\\u0e41\\u0e...\n",
       "7            Melbourne  @ashleighjayy_ Me @ this bitch https://t.co/8J...\n",
       "8            Melbourne  @AnaOLFan I \\u2764\\ufe0f you - I could never b...\n",
       "9               Sydney  @ihatejoelkim Welcome to Australia! Hoping you...\n",
       "10            Brisbane  Mea evolve conference @markbouris session. I L...\n",
       "11           Melbourne  First taste of Winter in Autumn https://t.co/0...\n",
       "12            Brisbane  \\u0643\\u064a\\u0641 \\u064a\\u0634\\u0648\\u0641 \\u...\n",
       "13              Sydney  #SelfieSunday #Lost #Tabby &amp; White/Caramel...\n",
       "14            Brisbane  @artsdesire Give her credit, she looks incredi...\n",
       "15              Sydney  @Pringster78 @Chadderbox2018 I kind of do the ...\n",
       "16               Perth       @gramercypark It\\u2019s great isn\\u2019t it?\n",
       "17              Sydney  https://t.co/RT37Cj5KRF  I date her, not MJ xo...\n",
       "18               Perth  @AussieAndyCx @ImScaredq Deadest Kane would de...\n",
       "19              Sydney  if i see got spoilers (and esp not tagged or a...\n",
       "20           Melbourne  \\u0639\\u064e\\u0644\\u064a\\u0643 \\u0639\\u0648\\u0...\n",
       "21               Perth           @pleaseuseaussie @RodS108443078 Hope so.\n",
       "22            Brisbane      x2 babe \\ud83d\\ude2b biggest scam I tell you.\n",
       "23           Melbourne  @idgimaddy @EXOGlobal @B_hundred_Hyun @1992050...\n",
       "24               Perth                                        sbren sbeve\n",
       "25              Sydney  Grassy, bitter (Kiwi) hops, clean - Drinking a...\n",
       "26           Melbourne  @FatherBob  Father Bob,  Thankyou I went to an...\n",
       "27            Brisbane  30th Birthday Dinner \\ud83e\\udd42\\ud83c\\udf88\\...\n",
       "28               Perth  @clementine_ford Us fellas were all drawn to i...\n",
       "29               Perth                            https://t.co/nejALF6216\n",
       "30           Melbourne  KOKO BUNNY! The Easter Bunny arrived at my doo...\n",
       "...                ...                                                ...\n",
       "103335       Melbourne  Wow, I\\u2019m not very good at getting the pic...\n",
       "103336       Melbourne  I gather the Govt wants to further clog up Pun...\n",
       "103337        Brisbane  Visualization is the key to success. Dream big...\n",
       "103338          Sydney  Countries I\\u2019ve Been To:  Fiji \\ud83c\\udde...\n",
       "103339        Brisbane  @TaodeHaas @carolemorrissey Yes i agree....   ...\n",
       "103340        Brisbane                                      @Wiley_Health\n",
       "103341        Brisbane                            Unbelievable @RiflePete\n",
       "103342       Melbourne  Thanks @TheJosephNaim for coming to see @Jerse...\n",
       "103343          Sydney  @HarryTrevor8888 @blondebonnie94 It is incompa...\n",
       "103344           Perth  The day before the budget, Sportsbet had odds ...\n",
       "103345           Perth  @sophie_walsh9 Where was this Sophie ? He was ...\n",
       "103346       Melbourne  People who are not scientists but who question...\n",
       "103347          Sydney  My mom came up to visit this morning and to ru...\n",
       "103348       Melbourne           TS7 \\u2764\\ufe0f https://t.co/OQOaAft5Fg\n",
       "103349       Melbourne  @iamtheoracle I know, I know. Those people are...\n",
       "103350       Melbourne  @StephanieLoveUK Already seen enough there\\u20...\n",
       "103351           Perth                @colinhussey22 gorillaz\\ud83d\\udc4c\n",
       "103352          Sydney  @insufferablscot @tinyelbows_ Part of me think...\n",
       "103353       Melbourne                          @CDeLaFuentez Lol exactly\n",
       "103354        Brisbane  @rmharmon2004 outside of the context ofwrestli...\n",
       "103355          Sydney                  @KirstyWebeck So SMASH it Kirsty!\n",
       "103356       Melbourne  \\u0e04\\u0e34\\u0e14\\u0e16\\u0e36\\u0e07\\u0e2b\\u0e...\n",
       "103357       Melbourne  @tbdalton5 That they are! I have had 2 in my l...\n",
       "103358           Perth  @GrayConnolly @GemmaTognini @ElizaJBarr You\\u2...\n",
       "103359       Melbourne  @australian @IzzyFolau very interesting that y...\n",
       "103360          Sydney                  Solzinho bom pra ir na praia hoje\n",
       "103361       Melbourne  @suzdavies13 Of me??? Thank you, surely you do...\n",
       "103362           Perth  @GraceSpelman have you heard of Dune? seen som...\n",
       "103363       Melbourne      Id bang you harder than an old Chevrolet door\n",
       "103364          Sydney  @Mezmfield @chief_comanche @Utopiana @fraser_a...\n",
       "\n",
       "[103360 rows x 2 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Import data into pandas format from original csv/tsv file formats for \n",
    "the top 10/50/100 train/test/dev data \n",
    "\"\"\"\n",
    "\n",
    "fdir = path.join(getcwd(), \"2019S1-proj2-datah\")\n",
    "\n",
    "# train_data = \"train-top10.csv\"\n",
    "# test_data = \"test-top10.csv\"\n",
    "# dev_data = \"dev-top10.csv\"\n",
    "\n",
    "train_data = \"train-raw.tsv\"\n",
    "test_data = \"test-raw.tsv\"\n",
    "dev_data = \"dev-raw.tsv\"\n",
    "\n",
    "train_fpath = path.join(fdir, train_data)\n",
    "test_fpath = path.join(fdir, test_data)\n",
    "dev_fpath = path.join(fdir, dev_data)\n",
    "\n",
    "train = pd.read_csv(train_fpath, sep=\"\\t\", index_col=\"Instance_ID\", encoding=\"utf_8\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ['Text']\n",
    "output = 'Location'\n",
    "x = train[inputs]\n",
    "y = train[output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTagger:\n",
    "    _FEATURE_SELECTION = [\"info_gain_ratio\", \"word_locality_heuristic\", 'tf_idf']\n",
    "    _VOTING_STRATEGY = [\"simple_voting\", \"bagging\", \"stacking\", \"random_forest\", \"boosting\"]\n",
    "    _CLASSIFIERS = [\"Zero-R\", \"One-R\", \"Decision-Tree\", \"MultinomialNB\", \"SVM\", \"SemiSupervised\", \"KNN\"]\n",
    "    _EVALUATION_METRIC = [\"accuracy\", \"precision\", \"recall\"]\n",
    "#     _EVALUATION METHOD = [\"Cross-Validation\"]\n",
    "                          \n",
    "    def __init__(self, inputs, target, classifier_set=[\"MultinomialNB\"], voting_strategy=\"simple_voting\", feature_selection=\"tf_idf\", eval_metric=\"accuracy\" seed=500):\n",
    "        self.inputs = inputs\n",
    "        self.target = target\n",
    "        self.exclude = set()\n",
    "        self.classifier_set = classifier_set\n",
    "        self.voting_strategy = voting_strategy\n",
    "        self.feature_selection = feature_selection\n",
    "        self.eval_metric = eval_metric\n",
    "        self.classifier_set = self._combine_classifier_set(classifier_set) # MultinomialNB, SVM\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        self.vectorizer = TfidfVectorizer(stop_words=set(stopwords.words('english')), preprocessor=self.filter)\n",
    "        np.random.seed(seed)\n",
    "                \n",
    "    def _combine_classifier_set(self, classifiers):\n",
    "        classifier_set = defaultdict()\n",
    "        \n",
    "        for classifer in classifiers:\n",
    "            if GeoTagger._CLASSIFIERS.index(classifier) == 0:\n",
    "                classifier_set[classifier] = DummyClassifier(strategy='most_frequent')\n",
    "            elif GeoTagger._CLASSIFIERS.index(classifier) == 1:\n",
    "                classifier_set[classifier] = DecisionTreeClassifier(max_depth=1, criterion=\"entropy\")\n",
    "            elif GeoTagger._CLASSIFIERS.index(classifier)==2:\n",
    "                classifier_set[classifier] = DecisionTreeClassifier(max_depth=None, criterion=\"entropy\")\n",
    "            elif GeoTagger._CLASSIFIERS.index(classifier)==3:\n",
    "                classifier_set[classifier] = MultinomialNB()\n",
    "        return classifier_set\n",
    "        \n",
    "#     def predict(self, x, y):\n",
    "#         predictions = pd.DataFrame()\n",
    "#         for name, classifier in self.classifier_set.items():\n",
    "#             classifier_prediction = classifier.predict(x, y)\n",
    "#             predictions[name] = classifier_predictions\n",
    "        \n",
    "    def train_baseline(self, X, y, classifier):\n",
    "        \"\"\"\n",
    "        trains a single classifier given the training data and their corresponding class labels\n",
    "        \"\"\"\n",
    "        print(GeoTagger._CLASSIFIERS.index(classifier))\n",
    "        \n",
    "        if GeoTagger._CLASSIFIERS.index(classifier)==0:\n",
    "            clf.fit(X, y)\n",
    "        elif GeoTagger._CLASSIFIERS.index(classifier)==1:\n",
    "            clf = DecisionTreeClassifier(max_depth=1, criterion=\"entropy\")\n",
    "            clf.fit(X,y)\n",
    "        elif GeoTagger._CLASSIFIERS.index(classifier)==2:\n",
    "            clf.fit(X,y)\n",
    "        elif GeoTagger._CLASSIFIERS.index(classifier)==3:\n",
    "            clf.fit(X,y)\n",
    "            print(clf)\n",
    "        return clf\n",
    "    \n",
    "    def combine_classifiers(self, X, y, classifiers):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, clf, X, classifier_type):\n",
    "        print(clf)\n",
    "#         print(X)\n",
    "#         print(classifier_type)\n",
    "        \n",
    "        if GeoTagger._CLASSIFIERS.index(classifier_type)==0:\n",
    "            predictions = clf.predict(X)\n",
    "        elif GeoTagger._CLASSIFIERS.index(classifier_type)==1:\n",
    "            predictions = clf.predict(X)\n",
    "        elif GeoTagger._CLASSIFIERS.index(classifier_type)==2:\n",
    "            predictions = clf.predict(X)\n",
    "        elif GeoTagger._CLASSIFIERS.index(classifier_type)==3:\n",
    "            print(\"kucing\")\n",
    "            predictions = clf.predict(X)\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, ybar, y, metric):\n",
    "        #TODO: eval method\n",
    "        \n",
    "        if GeoTagger._EVALUATION_METRIC.index(metric) == 0:\n",
    "            score = accuracy_score(ybar, y)\n",
    "        if GeoTagger._EVALUATION_METRIC.index(metric) == 1:\n",
    "            score = accuracy_score(ybar, y)\n",
    "        if GeoTagger._EVALUATION_METRIC.index(metric) == 2:\n",
    "            score = accuracy_score(ybar, y)\n",
    "        if GeoTagger._EVALUATION_METRIC.index(metric) == 3:\n",
    "            score = accuracy_score(ybar, y)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def preprocess(self, x, y, train=False):\n",
    "        \"\"\"\n",
    "         - Filter rare words (urls, typos rare names, punctuation symbols)\n",
    "         - calculate word frequencies \n",
    "         - metadata\n",
    "        \"\"\"\n",
    "        if train:\n",
    "            location_word_list = {label: '' for label in self.classes}\n",
    "            for index in x.index:\n",
    "                location_word_list[y.iloc[index]] += x.iloc[index] + \" \"\n",
    "            print(location_word_list.values())\n",
    "            features = self.vectorizer.fit_transform(location_word_list.values)\n",
    "        else:\n",
    "            features = self.vectoriser.transform(x[self.inputs])\n",
    "        return features     \n",
    "        \n",
    "    def feature_selection(self, x):\n",
    "        \"\"\"\n",
    "        (1) Information Gain Ratio (IGR) - across all states S, is \n",
    "            defined as the ratio between its information gain value IG, \n",
    "            which measures the decrease in class entropy H that w brings,\n",
    "            and its intrinsic entropy IV, which measures the entropy of \n",
    "            the presence versus the absence of that word\n",
    "            \n",
    "        (2) Word Locality Heuristic (WLH) - promotes words primarily \n",
    "            associated with one location. measure the probability of \n",
    "            a word occurring in a state, divided by its probability to \n",
    "            appear in any state. Then, for a given word w, we define the \n",
    "            WLH as the maximum such probability across all the states S\n",
    "        \"\"\"\n",
    "        if self.feature_selection == \"IGR\":\n",
    "#             return information_gain_ratio(x)\n",
    "             return\n",
    "        elif self.feature_selection == \"WLH\":\n",
    "            return word_locality_weight(x)\n",
    "#         elif seld\n",
    "        else:\n",
    "            print(\"Invalid Feature Selection method: {}. Choose one of \\\n",
    "            ({})\".format(self.feature_selection, \", \".join(GeoTagger._FEATURE_SELECTION)))\n",
    "        \n",
    "    def word_locality_weight(self, x):\n",
    "        \"\"\"\n",
    "        calculate frequencies of data \n",
    "        Measure frequency and divide by sum of freqencies of all words\n",
    "        \"\"\"\n",
    "    \n",
    "    def information_gain_ratio(self, x):\n",
    "#         return info_gain_ratio\n",
    "        pass\n",
    "        \n",
    "    def filter(self, text):\n",
    "        filtered = [self.filter_word(w) for w in text.split() if w not in self.stopwords]\n",
    "        return ' '.join(filtered)\n",
    "        \n",
    "    def filter_word(self, word):\n",
    "        word = word.lower()\n",
    "        # extract keywords from hashtag \n",
    "        if self._is_hyperlink(word):\n",
    "            return ''\n",
    "        elif self._is_hashtag(word):\n",
    "            word = self._process_hashtag(word)\n",
    "        # potentially cross-reference individuals mentioned? or discard\n",
    "        elif self._is_mention(word):\n",
    "            word = self._process_mention(word)\n",
    "        # remove ascii characters \n",
    "        else:\n",
    "            word = self._ascii_to_unicode(word)\n",
    "            word = self._word_stem(word)\n",
    "            # .decode(\"unicode_escape\").encode('utf-8')\n",
    "            word = re.sub(r'[^\\w\\s]',' ', word)\n",
    "#         print(word)\n",
    "        return word\n",
    "                \n",
    "    def _is_hashtag(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"#\"\n",
    "    \n",
    "    def _is_mention(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"@\"\n",
    "    \n",
    "    def _is_hyperlink(self, word):\n",
    "        return validators.url(word)\n",
    "    \n",
    "    def _process_hashtag(self, word):\n",
    "        return word[1:]\n",
    "    \n",
    "    def _process_mention(self, word):\n",
    "        return word[1:]\n",
    "    \n",
    "    def _ascii_to_unicode(self, word):\n",
    "        for uescape in re.findall(r'(\\\\u[0-9a-z]{4})', word):\n",
    "            try:\n",
    "#                 print(uescape.encode('utf-8').decode('unicode-escape'), type(uescape.encode('utf-8').decode('unicode-escape')))\n",
    "#                 word = re.sub(uescape, uescape.encode('utf-8'), word)\n",
    "#                 print(word)\n",
    "#                 print(uescape, type(uescape))\n",
    "                word = re.sub(uescape, '', word)  \n",
    "            except (UnicodeDecodeError, Exception):\n",
    "                print(\"Failed to decode: {}\".format(uescape))\n",
    "        return word\n",
    "    \n",
    "    def _word_stem(self, word):\n",
    "        return self.stemmer.stem(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_metric' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-c362231b6f7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGeoTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m gt = GeoTagger(\n\u001b[1;32m      5\u001b[0m     \u001b[0mclassifier_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"MultinomialNB\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-33-0252c1675ecf>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, target, classifier_set, voting_strategy, feature_selection, seed)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvoting_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvoting_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_selection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_set\u001b[0m \u001b[0;31m# MultinomialNB, SVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_metric' is not defined"
     ]
    }
   ],
   "source": [
    "gt = GeoTagger(inputs[0], output)\n",
    "print(x)\n",
    "\n",
    "gt = GeoTagger(\n",
    "    classifier_set = [\"MultinomialNB\"], \n",
    "    voting_strategy = \"simple_voting\", \n",
    "    eval_metric=\"accuracy\", \n",
    "    seed=500\n",
    ")\n",
    "\n",
    "x_features = gt.train(x_train, y_train)\n",
    "\n",
    "#predict the class labels of a set of test data\n",
    "ybar = self.predict(clf, x_dev)\n",
    "        \n",
    "#evaluate classifier performance\n",
    "score = self.evaluate(ybar, dev_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Resources\n",
    "- https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
