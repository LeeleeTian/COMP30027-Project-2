{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Build Baseline Location Classifier\\nApproaches:\\n- instance representation in form of 'bag' of words \\n    - features: word frequencies, metadata \\n    - exclude rare words in data set (used by less than 3 users )\\n- gramatical structure with NLP\\n- model instances in terms of authors instead of documents \\n\\n\\n- Baseline Classifier: Naive Bayes Model ()\\n\\n\""
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Build Baseline Location Classifier\n",
    "Approaches:\n",
    "- instance representation in form of 'bag' of words \n",
    "    - features: word frequencies, metadata \n",
    "    - exclude rare words in data set (used by less than 3 users )\n",
    "- gramatical structure with NLP\n",
    "- model instances in terms of authors instead of documents \n",
    "\n",
    "\n",
    "- Baseline Classifier: Naive Bayes Model ()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from os import path, getcwd\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re, string\n",
    "import validators\n",
    "from info_gain.info_gain import info_gain_ratio\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import data into pandas format from original csv/tsv file formats for \n",
    "the top 10/50/100 train/test/dev data \n",
    "\"\"\"\n",
    "\n",
    "fdir = path.join(getcwd(), \"2019S1-proj2-datah\")\n",
    "\n",
    "train_data = \"train-top10.csv\"\n",
    "test_data = \"test-top10.csv\"\n",
    "dev_data = \"dev-top10.csv\"\n",
    "\n",
    "train_fpath = path.join(fdir, train_data)\n",
    "test_fpath = path.join(fdir, test_data)\n",
    "dev_fpath = path.join(fdir, dev_data)\n",
    "\n",
    "train = pd.read_csv(train_fpath, sep=\"\\t\", index_col=\"Instance_ID\", encoding=\"utf_8\", dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ['Text']\n",
    "output = 'Location'\n",
    "x_train = train[inputs]\n",
    "y_train = train[output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTagger:\n",
    "    _FEATURE_SELECTION = [\"info_gain_ratio\", \"word_locality_heuristic\", 'chi_squared', 'mutial_info']\n",
    "    _VOTING_STRATEGY = [\"simple_voting\", \"bagging\", \"stacking\", \"random_forest\", \"boosting\"]\n",
    "    _CLASSIFIERS = [\"Zero-R\", \"One-R\", \"Decision-Tree\", \"MultinomialNB\", \"SVM\", \"SemiSupervised\", \"KNN\"]\n",
    "    _EVALUATION_METRIC = [\"Accuracy\", \"Precision\", \"Recall\"]\n",
    "#     _EVALUATION METHOD = [\"Cross-Validation\"]\n",
    "                          \n",
    "    def __init__(self, inputs, target, classifier_set=[\"MultinomialNB\"], voting_strategy=\"simple_voting\", feature_selection=\"IGR\", seed=500):\n",
    "        self.inputs = inputs\n",
    "        self.target = target\n",
    "        self.exclude = set()\n",
    "        self.classifier_set = classifier_set # MultinomialNB, SVM\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        self.vectorizer = TfidfVectorizer(stop_words=\"english\", preprocessor=self.filter)\n",
    "        np.random.seed(seed)\n",
    "        self.train_x = train_x\n",
    "        self.train_y= train_y\n",
    "        self.dev_x = dev_x\n",
    "        self.dev_y= dev_y\n",
    "            \n",
    "        #train classifier\n",
    "        if len(self.classifier_set)>1: #classifier combination\n",
    "            clf = self.combine_classifiers(self.train_x, self.train_y, self.classifier_set)\n",
    "        else: #baseline classifier\n",
    "            clf = self.train_baseline(self.train_x, self.train_y, self.classifier_set)\n",
    "        \n",
    "        #predict the class labels of a set of test data\n",
    "        ybar = self.predict(clf, self.dev_x, self.classifier_set)\n",
    "        \n",
    "        #evaluate classifier performance\n",
    "        score = self.evaluate(ybar, self.dev_y, self.eval_metric)\n",
    "        \n",
    "        print(\"score\")\n",
    "    def train_baseline(self, X, y, classifier):\n",
    "        \"\"\"\n",
    "        trains a single classifier given the training data and their corresponding class labels\n",
    "        \"\"\"\n",
    "        if GeoTagger._CLASSIFIERS.index(classifier)==0:\n",
    "            clf = DummyClassifier(strategy='most_frequent')\n",
    "            clf.fit(X, y)\n",
    "        elif GeoTagger._CLASSIFIERS.index(classifier)==1:\n",
    "            clf = DecisionTreeClassifier(max_depth=1, criterion=\"entropy\")\n",
    "            clf.fit(X,y)\n",
    "        elif GeoTagger._CLASSIFIERS.index(classifier)==2:\n",
    "            clf = DecisionTreeClassifier(max_depth=1, criterion=\"entropy\")\n",
    "            clf.fit(X,y)\n",
    "        elif GeoTagger._CLASSIFIERS.index(classifier)==3:\n",
    "            clf = MultinomialNB()\n",
    "            clf.fit(X,y)\n",
    "        return clf\n",
    "    \n",
    "    def combine_classifiers(self, X, y, classifiers):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, clf, X, classifier_type):\n",
    "        print(clf)\n",
    "#         print(X)\n",
    "#         print(classifier_type)\n",
    "        \n",
    "        if GeoTagger._CLASSIFIERS.index(classifier_type)==0:\n",
    "            predictions = clf.predict(X)\n",
    "        elif GeoTagger._CLASSIFIERS.index(classifier_type)==1:\n",
    "            predictions = clf.predict(X)\n",
    "        elif GeoTagger._CLASSIFIERS.index(classifier_type)==2:\n",
    "            predictions = clf.predict(X)\n",
    "        elif GeoTagger._CLASSIFIERS.index(classifier_type)==3:\n",
    "            print(\"kucing\")\n",
    "            predictions = clf.predict(X)\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, ybar, y, metric):\n",
    "        #TODO: eval method\n",
    "        \n",
    "        if GeoTagger._EVALUATION_METRIC.index(metric) == 0:\n",
    "            score = accuracy_score(ybar, y)\n",
    "        if GeoTagger._EVALUATION_METRIC.index(metric) == 1:\n",
    "            score = accuracy_score(ybar, y)\n",
    "        if GeoTagger._EVALUATION_METRIC.index(metric) == 2:\n",
    "            score = accuracy_score(ybar, y)\n",
    "        if GeoTagger._EVALUATION_METRIC.index(metric) == 3:\n",
    "            score = accuracy_score(ybar, y)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def preprocess(self, x, train=False):\n",
    "        \"\"\"\n",
    "         - Filter rare words (urls, typos rare names, punctuation symbols)\n",
    "         - calculate word frequencies \n",
    "         - metadata\n",
    "        \"\"\"\n",
    "        if train:\n",
    "            features = self.vectorizer.fit_transform(x[self.inputs].tolist())\n",
    "        else:\n",
    "            features = self.vectoriser.transform(x[self.inputs])\n",
    "        return features     \n",
    "        \n",
    "    def feature_selection(self, x):\n",
    "        \"\"\"\n",
    "        (1) Information Gain Ratio (IGR) - across all states S, is \n",
    "            defined as the ratio between its information gain value IG, \n",
    "            which measures the decrease in class entropy H that w brings,\n",
    "            and its intrinsic entropy IV, which measures the entropy of \n",
    "            the presence versus the absence of that word\n",
    "            \n",
    "        (2) Word Locality Heuristic (WLH) - promotes words primarily \n",
    "            associated with one location. measure the probability of \n",
    "            a word occurring in a state, divided by its probability to \n",
    "            appear in any state. Then, for a given word w, we define the \n",
    "            WLH as the maximum such probability across all the states S\n",
    "        \"\"\"\n",
    "        if self.feature_selection == \"IGR\":\n",
    "#             return information_gain_ratio(x)\n",
    "             return\n",
    "        elif self.feature_selection == \"WLH\":\n",
    "            return word_locality_weight(x)\n",
    "#         elif seld\n",
    "        else:\n",
    "            print(\"Invalid Feature Selection method: {}. Choose one of \\\n",
    "            ({})\".format(self.feature_selection, \", \".join(GeoTagger._FEATURE_SELECTION)))\n",
    "        \n",
    "    def word_locality_weight(self, x):\n",
    "        \"\"\"\n",
    "        calculate frequencies of data \n",
    "        Measure frequency and divide by sum of freqencies of all words\n",
    "        \"\"\"\n",
    "    \n",
    "    def information_gain_ratio(self, x):\n",
    "#         return info_gain_ratio\n",
    "        pass\n",
    "        \n",
    "    def filter(self, text):\n",
    "        filtered = [self.filter_word(w) for w in text.split() if w not in self.stopwords]\n",
    "        return ' '.join(filtered)\n",
    "        \n",
    "    def filter_word(self, word):\n",
    "        word = word.lower()\n",
    "        # extract keywords from hashtag \n",
    "        if self._is_hyperlink(word):\n",
    "            return ''\n",
    "        elif self._is_hashtag(word):\n",
    "            word = self._process_hashtag(word)\n",
    "        # potentially cross-reference individuals mentioned? or discard\n",
    "        elif self._is_mention(word):\n",
    "            word = self._process_mention(word)\n",
    "        # remove ascii characters \n",
    "        else:\n",
    "            word = self._ascii_to_unicode(word)\n",
    "            word = self._word_stem(word)\n",
    "            # .decode(\"unicode_escape\").encode('utf-8')\n",
    "            word = re.sub(r'[^\\w\\s]',' ', word)\n",
    "#         print(word)\n",
    "        return word\n",
    "                \n",
    "    def _is_hashtag(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"#\"\n",
    "    \n",
    "    def _is_mention(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"@\"\n",
    "    \n",
    "    def _is_hyperlink(self, word):\n",
    "        return validators.url(word)\n",
    "    \n",
    "    def _process_hashtag(self, word):\n",
    "        return word[1:]\n",
    "    \n",
    "    def _process_mention(self, word):\n",
    "        return word[1:]\n",
    "    \n",
    "    def _ascii_to_unicode(self, word):\n",
    "        for uescape in re.findall(r'(\\\\u[0-9a-z]{4})', word):\n",
    "            try:\n",
    "#                 print(uescape.encode('utf-8').decode('unicode-escape'), type(uescape.encode('utf-8').decode('unicode-escape')))\n",
    "#                 word = re.sub(uescape, uescape.encode('utf-8'), word)\n",
    "#                 print(word)\n",
    "#                 print(uescape, type(uescape))\n",
    "                word = re.sub(uescape, '', word)  \n",
    "            except (UnicodeDecodeError, Exception):\n",
    "                print(\"Failed to decode: {}\".format(uescape))\n",
    "        return word\n",
    "    \n",
    "    def _word_stem(self, word):\n",
    "        return self.stemmer.stem(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to decode: \\urban\n",
      "Failed to decode: \\unnec\n",
      "Failed to decode: \\union\n",
      "Failed to decode: \\unint\n",
      "Failed to decode: \\unrel\n",
      "Failed to decode: \\unsub\n",
      "Failed to decode: \\unemp\n",
      "Failed to decode: \\unsub\n",
      "Failed to decode: \\upper\n",
      "Failed to decode: \\unkno\n",
      "  (0, 115436)\t0.27066456056875327\n",
      "  (0, 116424)\t0.7157332853353271\n",
      "  (0, 66820)\t0.3655113195722454\n",
      "  (0, 119227)\t0.42967926362337056\n",
      "  (0, 10273)\t0.3102317933285641\n",
      "  (1, 103244)\t0.2886738403593304\n",
      "  (1, 24795)\t0.24155651823295546\n",
      "  (1, 24663)\t0.29744953801359963\n",
      "  (1, 68502)\t0.3373328078981321\n",
      "  (1, 98123)\t0.3171781951057145\n",
      "  (1, 1758)\t0.24619520847769885\n",
      "  (1, 97839)\t0.3687514943177701\n",
      "  (1, 12662)\t0.3159311852121531\n",
      "  (1, 8171)\t0.3063743833276534\n",
      "  (1, 117583)\t0.4064218033078263\n",
      "  (2, 47477)\t0.2370729787973891\n",
      "  (2, 79386)\t0.2019023730941069\n",
      "  (2, 63953)\t0.22600860812554002\n",
      "  (2, 74211)\t0.3639175305844795\n",
      "  (2, 106111)\t0.424829004652617\n",
      "  (2, 94400)\t0.2935959916633253\n",
      "  (2, 28165)\t0.27995643696306455\n",
      "  (2, 30742)\t0.2870176303631887\n",
      "  (2, 3934)\t0.32337465746419003\n",
      "  (2, 99042)\t0.3387241951227804\n",
      "  :\t:\n",
      "  (103356, 100343)\t0.3148355466380959\n",
      "  (103356, 23400)\t0.4030436357306584\n",
      "  (103356, 69417)\t0.4051132717659284\n",
      "  (103356, 122094)\t0.38387082261496325\n",
      "  (103356, 100571)\t0.45618474372623785\n",
      "  (103357, 79386)\t0.2137339569742582\n",
      "  (103357, 92622)\t0.2776632514947095\n",
      "  (103357, 101530)\t0.29276573420895885\n",
      "  (103357, 45921)\t0.3064456106512332\n",
      "  (103357, 20680)\t0.3737060855249342\n",
      "  (103357, 122863)\t0.31126203156295257\n",
      "  (103357, 32225)\t0.45626428180804907\n",
      "  (103357, 43256)\t0.5028745782859907\n",
      "  (103358, 31080)\t0.3720016167530209\n",
      "  (103358, 45282)\t0.4077617774553645\n",
      "  (103358, 75906)\t0.28364656089264684\n",
      "  (103358, 11629)\t0.39520004967361466\n",
      "  (103358, 48768)\t0.40541394256651075\n",
      "  (103358, 20907)\t0.5425368322833065\n",
      "  (103359, 115436)\t0.4067745263437195\n",
      "  (103359, 116544)\t0.8782840591706967\n",
      "  (103359, 39457)\t0.09368036337744538\n",
      "  (103359, 118180)\t0.12610569523657592\n",
      "  (103359, 20992)\t0.13445705060117993\n",
      "  (103359, 67564)\t0.14280840596578398\n"
     ]
    }
   ],
   "source": [
    "gt = GeoTagger(inputs[0], output)\n",
    "x_features = gt.train(x_train, y_train)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Resources\n",
    "- https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
