{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Build Baseline Location Classifier\\nApproaches:\\n- instance representation in form of 'bag' of words \\n    - features: word frequencies, metadata \\n    - exclude rare words in data set (used by less than 3 users )\\n- gramatical structure with NLP\\n- model instances in terms of authors instead of documents \\n\\n\\n- Baseline Classifier: Naive Bayes Model ()\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Build Baseline Location Classifier\n",
    "Approaches:\n",
    "- instance representation in form of 'bag' of words \n",
    "    - features: word frequencies, metadata \n",
    "    - exclude rare words in data set (used by less than 3 users )\n",
    "- gramatical structure with NLP\n",
    "- model instances in terms of authors instead of documents \n",
    "\n",
    "\n",
    "- Baseline Classifier: Naive Bayes Model ()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time \n",
    "from os import path, getcwd\n",
    "from collections import defaultdict\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance_ID</th>\n",
       "      <th>Location</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103340</th>\n",
       "      <td>103345</td>\n",
       "      <td>Perth</td>\n",
       "      <td>@sophie_walsh9 Where was this Sophie ? He was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103341</th>\n",
       "      <td>103346</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>People who are not scientists but who question...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103342</th>\n",
       "      <td>103347</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>My mom came up to visit this morning and to ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103343</th>\n",
       "      <td>103348</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>TS7 \\u2764\\ufe0f https://t.co/OQOaAft5Fg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103344</th>\n",
       "      <td>103349</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>@iamtheoracle I know, I know. Those people are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103345</th>\n",
       "      <td>103350</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>@StephanieLoveUK Already seen enough there\\u20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103346</th>\n",
       "      <td>103351</td>\n",
       "      <td>Perth</td>\n",
       "      <td>@colinhussey22 gorillaz\\ud83d\\udc4c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103347</th>\n",
       "      <td>103352</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>@insufferablscot @tinyelbows_ Part of me think...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103348</th>\n",
       "      <td>103353</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>@CDeLaFuentez Lol exactly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103349</th>\n",
       "      <td>103354</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>@rmharmon2004 outside of the context ofwrestli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103350</th>\n",
       "      <td>103355</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>@KirstyWebeck So SMASH it Kirsty!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103351</th>\n",
       "      <td>103356</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>\\u0e04\\u0e34\\u0e14\\u0e16\\u0e36\\u0e07\\u0e2b\\u0e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103352</th>\n",
       "      <td>103357</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>@tbdalton5 That they are! I have had 2 in my l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103353</th>\n",
       "      <td>103358</td>\n",
       "      <td>Perth</td>\n",
       "      <td>@GrayConnolly @GemmaTognini @ElizaJBarr You\\u2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103354</th>\n",
       "      <td>103359</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>@australian @IzzyFolau very interesting that y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103355</th>\n",
       "      <td>103360</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Solzinho bom pra ir na praia hoje</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103356</th>\n",
       "      <td>103361</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>@suzdavies13 Of me??? Thank you, surely you do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103357</th>\n",
       "      <td>103362</td>\n",
       "      <td>Perth</td>\n",
       "      <td>@GraceSpelman have you heard of Dune? seen som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103358</th>\n",
       "      <td>103363</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Id bang you harder than an old Chevrolet door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103359</th>\n",
       "      <td>103364</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>@Mezmfield @chief_comanche @Utopiana @fraser_a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Instance_ID   Location  \\\n",
       "103340       103345      Perth   \n",
       "103341       103346  Melbourne   \n",
       "103342       103347     Sydney   \n",
       "103343       103348  Melbourne   \n",
       "103344       103349  Melbourne   \n",
       "103345       103350  Melbourne   \n",
       "103346       103351      Perth   \n",
       "103347       103352     Sydney   \n",
       "103348       103353  Melbourne   \n",
       "103349       103354   Brisbane   \n",
       "103350       103355     Sydney   \n",
       "103351       103356  Melbourne   \n",
       "103352       103357  Melbourne   \n",
       "103353       103358      Perth   \n",
       "103354       103359  Melbourne   \n",
       "103355       103360     Sydney   \n",
       "103356       103361  Melbourne   \n",
       "103357       103362      Perth   \n",
       "103358       103363  Melbourne   \n",
       "103359       103364     Sydney   \n",
       "\n",
       "                                                     Text  \n",
       "103340  @sophie_walsh9 Where was this Sophie ? He was ...  \n",
       "103341  People who are not scientists but who question...  \n",
       "103342  My mom came up to visit this morning and to ru...  \n",
       "103343           TS7 \\u2764\\ufe0f https://t.co/OQOaAft5Fg  \n",
       "103344  @iamtheoracle I know, I know. Those people are...  \n",
       "103345  @StephanieLoveUK Already seen enough there\\u20...  \n",
       "103346                @colinhussey22 gorillaz\\ud83d\\udc4c  \n",
       "103347  @insufferablscot @tinyelbows_ Part of me think...  \n",
       "103348                          @CDeLaFuentez Lol exactly  \n",
       "103349  @rmharmon2004 outside of the context ofwrestli...  \n",
       "103350                  @KirstyWebeck So SMASH it Kirsty!  \n",
       "103351  \\u0e04\\u0e34\\u0e14\\u0e16\\u0e36\\u0e07\\u0e2b\\u0e...  \n",
       "103352  @tbdalton5 That they are! I have had 2 in my l...  \n",
       "103353  @GrayConnolly @GemmaTognini @ElizaJBarr You\\u2...  \n",
       "103354  @australian @IzzyFolau very interesting that y...  \n",
       "103355                  Solzinho bom pra ir na praia hoje  \n",
       "103356  @suzdavies13 Of me??? Thank you, surely you do...  \n",
       "103357  @GraceSpelman have you heard of Dune? seen som...  \n",
       "103358      Id bang you harder than an old Chevrolet door  \n",
       "103359  @Mezmfield @chief_comanche @Utopiana @fraser_a...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdir = path.join(getcwd(), \"2019S1-proj2-datah\")\n",
    "\n",
    "train_data = \"train-raw.tsv\"\n",
    "test_data = \"test-raw.tsv\"\n",
    "dev_data = \"dev-raw.tsv\"\n",
    "\n",
    "train_fpath = path.join(fdir, train_data)\n",
    "\n",
    "train = pd.read_csv(train_fpath, sep=\"\\t\")\n",
    "classes = train['Location'].unique()\n",
    "\n",
    "data.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GeoTagger:\n",
    "    def __init__(self, classes, classifier=\"MultinomialNB\", feature_selection=\"IGR\"):\n",
    "        self.classes = classes\n",
    "        self.exclude = set()\n",
    "        self.class_feature_sets = {label: defaultdict() for label in class}\n",
    "        self.classifier = classifier # MultinomialNB, SVM\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.\n",
    "        \n",
    "    def train(self, x, y):\n",
    "        pass\n",
    "    \n",
    "    def test(self, x, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x):\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, x, y):\n",
    "        pass\n",
    "    \n",
    "    def preprocess(self, x, y):\n",
    "        \"\"\"\n",
    "         - Filter rare words (urls, typos rare names, punctuation symbols)\n",
    "         - calculate word frequencies \n",
    "         - metadata\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def filter(self, x):\n",
    "        \"\"\"\n",
    "        https://towardsdatascience.com/extracting-twitter-data-pre-processing-and-sentiment-analysis-using-python-3-0-7192bd8b47cf\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def feature_weight(self, x):\n",
    "        \"\"\"\n",
    "        calculate frequencies of data \n",
    "        Measure frequency and divide by sum of freqencies of all words\n",
    "        \"\"\"\n",
    "    def feature_selection(self, x):\n",
    "        \"\"\"\n",
    "        (1) Information Gain Ratio (IGR) - across all states S, is \n",
    "            defined as the ratio between its information gain value IG, \n",
    "            which measures the decrease in class entropy H that w brings,\n",
    "            and its intrinsic entropy IV, which measures the entropy of \n",
    "            the presence versus the absence of that word\n",
    "            \n",
    "        (2) Word Locality Heuristic (WLH) - promotes words primarily \n",
    "            associated with one location. measure the probability of \n",
    "            a word occurring in a state, divided by its probability to \n",
    "            appear in any state. Then, for a given word w, we define the \n",
    "            WLH as the maximum such probability across all the states S\n",
    "        \"\"\"\n",
    "    def filter_tweet(self, tweet):\n",
    "        word_tokens = word_tokenize(tweet)\n",
    "        \n",
    "        filtered = [filter_word(w) for w in word_tokens if w not in self.stopwords]\n",
    "        \n",
    "        return ' '.join(filtered)\n",
    "        \n",
    "    def filter_word(self, word):\n",
    "        if self._is_hashtag(w):\n",
    "            w = self.process_hashtag(w)\n",
    "        elif self._is_mention():\n",
    "            w = self.process_mention(w)\n",
    "        else:\n",
    "            re.sub(r'[^\\x00-\\x7f]', r' ', w)\n",
    "        return w\n",
    "                \n",
    "    def _is_hashtag(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"#\"\n",
    "    \n",
    "    def _process_hashtag(self, x):\n",
    "        pass\n",
    "    \n",
    "    def _is_mention(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"@\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
