{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "from os import path, getcwd, makedirs\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier, BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import validators\n",
    "import nltk\n",
    "from info_gain.info_gain import info_gain_ratio\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir = path.join(getcwd(), \"2019S1-proj2-datah\")\n",
    "\n",
    "train_data = \"train-raw.tsv\"\n",
    "test_data = \"test-raw.tsv\"\n",
    "dev_data = \"dev-raw.tsv\"\n",
    "\n",
    "train_fpath = path.join(fdir, train_data)\n",
    "test_fpath = path.join(fdir, test_data)\n",
    "dev_fpath = path.join(fdir, dev_data)\n",
    "\n",
    "train = pd.read_csv(train_fpath, encoding=\"utf_8\", delimiter=\"\\t\", index_col=\"Instance_ID\")\n",
    "test = pd.read_csv(test_fpath, encoding=\"utf_8\", delimiter=\"\\t\" , index_col=\"Instance_ID\")\n",
    "dev = pd.read_csv(dev_fpath, encoding=\"utf_8\", delimiter=\"\\t\" , index_col=\"Instance_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'Text'\n",
    "output = 'Location'\n",
    "x_train = train[inputs]\n",
    "y_train = train[output]\n",
    "x_test = test[inputs]\n",
    "y_test = test[output]\n",
    "x_dev = dev[inputs]\n",
    "y_dev = dev[output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTagger:\n",
    "    _FEATURE_SELECTION = [\"baseline_10\", \"baseline_50\", \"baseline_100\", \"info_gain_ratio\", \"word_locality_heuristic\", \"tf_icf\"]\n",
    "    _ENSEMBLE_STRATEGY = [\"simple_voting\", \"meta_classification\", \"bagging\", \"random_forest\", \"boosting\"]\n",
    "    _CLASSIFIERS = [\"Zero-R\", \"One-R\", \"Decision-Tree\", \"MultinomialNB\", \"LinearSVM\", \"SemiSupervised\"]\n",
    "    _EVALUATION_METRIC = [\"accuracy\", \"precision_recall_f-score_with_macro\", \"precision_recall_f-score_with_micro\"]\n",
    "    \n",
    "    def __init__(self, inputs, target, classifier_set=[\"MultinomialNB\"], ensemble_strategy=\"simple_voting\", feature_selection_method=\"baseline_100\", seed=500, combine_classifiers=False, n_features=400):\n",
    "        self.inputs = inputs\n",
    "        self.target = target\n",
    "        self.classifier_set = classifier_set\n",
    "        self.ensemble_strategy = ensemble_strategy\n",
    "        self.feature_selection_method = feature_selection_method\n",
    "        self.seed = seed\n",
    "        np.random.seed(seed)\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.combine_classifiers = combine_classifiers\n",
    "        self.classifier_set = self._generate_classifier_set(classifier_set)\n",
    "        self.combined_classifier = None if not self.combine_classifiers else self.generate_ensemble_classifier()\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        trains a classifier given the training data and their corresponding class labels\n",
    "        \"\"\"\n",
    "        self.classes = list(y.unique())\n",
    "        X = self.preprocess(X, y, train=True)\n",
    "        \n",
    "        if self.combine_classifiers:\n",
    "            self.combined_classifier.fit(X, y)\n",
    "        else:\n",
    "            for name, classifier in self.classifier_set.items():\n",
    "                classifier.fit(X, y)\n",
    "                self.save_model(name, classifier)\n",
    "        \n",
    "\n",
    "    def predict(self, X, load_model=False):\n",
    "        \"\"\"\n",
    "        predicts a set of classifiers given some development data\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        \n",
    "        if self.combine_classifiers:\n",
    "            return self.ensemble_predict(X, load_model)\n",
    "        else:\n",
    "            return self.base_predict(X, load_model)\n",
    "        \n",
    "    \n",
    "    def evaluate(self, ybar, y):\n",
    "        \"\"\"\n",
    "        evaluates a class' predictions given the correct class labels and an evaluation metric\n",
    "        \"\"\"\n",
    "#         if not metric in GeoTagger._EVALUATION_METRIC:\n",
    "#             print(\"Invalid Evaluation Metric: {}. Choose one of \\\n",
    "#             ({})\".format(metric, \", \".join(GeoTagger._EVALUATION_METRIC)))\n",
    "#             return\n",
    "            \n",
    "        score_set = defaultdict()\n",
    "        \n",
    "        if self.combine_classifiers:\n",
    "            classifiers = [self.ensemble_strategy, ]\n",
    "            ybar = pd.DataFrame(ybar, columns=classifiers, index = y.index)\n",
    "        else:\n",
    "            classifiers = self.classifier_set\n",
    "\n",
    "        for name, y_pred in ybar.items():\n",
    "            accuracy = accuracy_score(y, y_pred)\n",
    "            score_set[name] = accuracy \n",
    "#             report = classification_report(ybar, y, self.target)\n",
    "#             confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "        return score_set\n",
    "\n",
    "    def base_predict(self, x, load_model=False):\n",
    "        y_set = pd.DataFrame()\n",
    "\n",
    "        for name, classifier in self.classifier_set.items():\n",
    "            if load_model:\n",
    "                classifier = self.load_model(name)\n",
    "            classifier_prediction = classifier.predict(X)\n",
    "            y_set[name] = classifier_prediction\n",
    "                \n",
    "        return y_set\n",
    "    \n",
    "    def ensemble_predict(self, X, load_model=False):\n",
    "        if load_model:\n",
    "            self.combined_classifier = self.load_model(self.ensemble_strategy)\n",
    "        \n",
    "        return self.combined_classifier.predict(X, load_model)\n",
    "    \n",
    "    def cross_validation(self, X, y, metric):\n",
    "        score_set = defaultdict()\n",
    "        \n",
    "        X = self.preprocess(X)\n",
    "\n",
    "        if self.combine_classifiers:\n",
    "            classifiers = [self.ensemble_strategy, ]\n",
    "            ybar = pd.DataFrame(ybar, columns=classifiers, index = y.index)\n",
    "        else:\n",
    "            classifiers = self.classifier_set\n",
    "\n",
    "        for classifier in classifiers:\n",
    "            score_set[metric] = cross_validate(classifier, X, y, cv=10)\n",
    "        return score_set\n",
    "    \n",
    "    \n",
    "    def preprocess(self, X, y=None, train=False):\n",
    "        \"\"\"\n",
    "         - Filter rare words (urls, typos rare names, punctuation symbols)\n",
    "         - calculate word frequencies \n",
    "         - metadata\n",
    "        \"\"\"\n",
    "        X = self.filter(X)\n",
    "\n",
    "        if train:\n",
    "            self.feature_selection(X, y)\n",
    "        \n",
    "        X = self.bag_of_words(X)\n",
    "            \n",
    "        return X \n",
    "    \n",
    "    def bag_of_words(self, X):\n",
    "        _x = pd.DataFrame(\n",
    "            [[(word in text) for word in sorted(list(self.features))] for text in X.values],\n",
    "            index=X.index, \n",
    "            columns=self.features,\n",
    "            dtype=np.uint8\n",
    "        )        \n",
    "        return _x\n",
    "    \n",
    "    def generate_ensemble_classifier(self):\n",
    "        if not self.combine_classifiers:\n",
    "            return None\n",
    "        \n",
    "        if not self.ensemble_strategy in GeoTagger._ENSEMBLE_STRATEGY:\n",
    "            print(\"Invalid Ensemble Strategy Metric: {}. Choose one of \\\n",
    "            ({})\".format(metric, \", \".join(GeoTagger._ENSEMBLE_STRATEGY)))\n",
    "            return None\n",
    "        \n",
    "        if self.ensemble_strategy == \"simple_voting\":\n",
    "            combined_classifier = VotingClassifier(self.classifier_set.items(), 'hard')\n",
    "        elif self.ensemble_strategy == \"meta_classification\":\n",
    "            combined_classifier = MetaClassifier(self.classifier_set.items(), self.seed)\n",
    "        elif self.ensemble_strategy == \"bagging\":\n",
    "            base_classifier = DecisionTreeClassifier(max_features=None, max_leaf_nodes=999)\n",
    "            combined_classifier = BaggingClassifier(base_estimator=base_classifier, max_features=self.n_features, random_state=self.seed)\n",
    "        elif self.ensemble_strategy == \"random_forest\":\n",
    "            combined_classifier = RandomForestClassifier()\n",
    "        elif self.ensemble_strategy == \"boosting\":\n",
    "            combined_classifier = GradientBoostingClassifier()\n",
    "        \n",
    "        return combined_classifier\n",
    "        \n",
    "    \n",
    "    def _generate_classifier_set(self, classifiers):\n",
    "        classifier_set = defaultdict()\n",
    "        \n",
    "        for classifier in classifiers:\n",
    "            if not classifier in GeoTagger._CLASSIFIERS:\n",
    "                print(\"Invalid Classifier: {}. Choose one of \\\n",
    "                ({})\".format(classifier, \", \".join(GeoTagger._CLASSIFIERS)))\n",
    "                continue\n",
    "                \n",
    "            if classifier == \"Zero-R\":\n",
    "                classifier_set[classifier] = DummyClassifier(strategy='most_frequent', random_state=self.seed)\n",
    "            elif classifier == \"One-R\":\n",
    "                classifier_set[classifier] = DecisionTreeClassifier(max_depth=1, criterion=\"entropy\", random_state=self.seed)\n",
    "            elif classifier == \"Decision-Tree\":\n",
    "                classifier_set[classifier] = DecisionTreeClassifier(max_depth=None, criterion=\"entropy\", random_state=self.seed)\n",
    "            elif classifier == \"MultinomialNB\":\n",
    "                classifier_set[classifier] = MultinomialNB()\n",
    "            elif classifier == \"LinearSVM\":\n",
    "                classifier_set[classifier] = svm.LinearSVC(random_state=self.seed)\n",
    "            elif classifier == \"SemiSupervised\":\n",
    "                classifier_set[classifier] = LabelSpreading(kernel=\"knn\", n_neighbors=7, alpha=0.2)\n",
    "        return classifier_set\n",
    "                \n",
    "    def feature_selection(self, X, y):\n",
    "        \"\"\"\n",
    "        (1) Information Gain Ratio (IGR) - across all states S, is \n",
    "            defined as the ratio between its information gain value IG, \n",
    "            which measures the decrease in class entropy H that w brings,\n",
    "            and its intrinsic entropy IV, which measures the entropy of \n",
    "            the presence versus the absence of that word\n",
    "            \n",
    "        (2) Word Locality Heuristic (WLH) - promotes words primarily \n",
    "            associated with one location. measure the probability of \n",
    "            a word occurring in a state, divided by its probability to \n",
    "            appear in any state. Then, for a given word w, we define the \n",
    "            WLH as the maximum such probability across all the states S\n",
    "        \"\"\"\n",
    "        if self.feature_selection_method not in GeoTagger._FEATURE_SELECTION:\n",
    "            print(\"Invalid Feature Selection method: {}. Choose one of \\\n",
    "            ({})\".format(self.feature_selection_method, \", \".join(GeoTagger._FEATURE_SELECTION)))\n",
    "            return \n",
    "        \n",
    "        if self.feature_selection_method == \"baseline_10\":\n",
    "            self.baseline_heuristic(X, y, \"10\")\n",
    "        elif self.feature_selection_method == \"baseline_50\":\n",
    "            self.baseline_heuristic(X, y, \"50\")\n",
    "        elif self.feature_selection_method == \"baseline_100\":\n",
    "            self.baseline_heuristic(X, y, \"100\")\n",
    "        elif self.feature_selection_method == \"info_gain_ratio\":\n",
    "#             self.information_gain_ratio(x)\n",
    "            return\n",
    "        elif self.feature_selection_method == \"word_locality_heuristic\":\n",
    "            self.word_locality_weight(X, y)\n",
    "        elif self.feature_selection_method == \"tf_icf\":\n",
    "            self.term_frequency_inverse_city_frequency(X, y)\n",
    "\n",
    "    def baseline_heuristic(self, X, y, top_n):\n",
    "        feature_fpath = path.join(fdir, \"train-top\" + top_n + \".csv\")\n",
    "        \n",
    "        if not path.exists(feature_fpath):\n",
    "            print(\"Baseline Heuristic path {} does not exist\".format(feature_fpath))\n",
    "            return\n",
    "        \n",
    "        features = open(feature_fpath).readline()\n",
    "        features = features.split(\",\")\n",
    "        features.remove(\"Instance_ID\")\n",
    "        features.remove(\"Location\\n\")\n",
    "        self.features = set(features)\n",
    "\n",
    "    def word_locality_weight(self, X, y):\n",
    "        \"\"\"\n",
    "        calculate frequencies of data \n",
    "        Measure frequency and divide by sum of freqencies of all words\n",
    "        \"\"\"\n",
    "        locations = [location for location in self.classes]\n",
    "        locations.append('Total')\n",
    "        word_locality_features = {label: defaultdict(int) for label in locations}\n",
    "        word_locality_weight = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        for x_i, y_i in zip(X.index, y.index):\n",
    "            text = X.loc[x_i].split()\n",
    "            for word in text:\n",
    "                word_locality_features[y.loc[y_i]][word] += 1\n",
    "                word_locality_features[y.loc[y_i]]['Total'] += 1\n",
    "                word_locality_features['Total'][word] += 1\n",
    "                word_locality_features['Total']['Total'] += 1\n",
    "        \n",
    "        for location in self.classes:\n",
    "            for word in word_locality_features[location].keys():\n",
    "#                 if not word_locality_weight.get(word):\n",
    "#                     word_locality_weight[word] = defaultdict(int)\n",
    "                    \n",
    "                cond_word_prob = word_locality_features[location][word] / word_locality_features[location]['Total']\n",
    "                word_prob = word_locality_features['Total'][word] / word_locality_features['Total']['Total']\n",
    "                word_locality_weight[word][location] = cond_word_prob / word_prob\n",
    "        \n",
    "        features = []\n",
    "        for word in word_locality_weight:\n",
    "            max_score = max(word_locality_weight[word].items(), key=lambda item:item[1])[1]\n",
    "            features.append((word, max_score))\n",
    "            n_location_features = int(self.n_features / len(self.classes))\n",
    "        \n",
    "        features = sorted(features, key=lambda kv: kv[1], reverse=True)[:self.n_features] \n",
    "\n",
    "        self.features = set([feature[0] for feature in features])\n",
    "        print(sorted(list(self.features)))\n",
    "                \n",
    "    \n",
    "    def information_gain_ratio(self, X):\n",
    "#         return info_gain_ratio\n",
    "        pass\n",
    "    \n",
    "    def term_frequency_inverse_city_frequency(self, X, y):\n",
    "        vectorizer = TfidfVectorizer(stop_words=self.stop_words, max_features=self.n_features)\n",
    "\n",
    "        location_word_list = {label: '' for label in self.classes}\n",
    "\n",
    "        for x_i, y_i in zip(X.index, y.index):\n",
    "            location_word_list[y.loc[y_i]] += X.loc[x_i] + \" \"\n",
    "        \n",
    "        labels = location_word_list.keys()\n",
    "        corpus = location_word_list.values()\n",
    "        vectorizer.fit(corpus, labels)\n",
    "        self.features = set(vectorizer.get_feature_names())\n",
    "        \n",
    "    def filter(self, X):\n",
    "        return X.apply(self.filter_text)\n",
    "    \n",
    "    def filter_text(self, text):\n",
    "        return ' '.join(self.filter_word(w) for w in text.split())\n",
    "        \n",
    "    def filter_word(self, word):\n",
    "        word = word.lower()\n",
    "        # extract keywords from hashtag \n",
    "\n",
    "        if self._is_hyperlink(word):\n",
    "            return ''\n",
    "        elif self._is_hashtag(word):\n",
    "            word = self._process_hashtag(word)\n",
    "        # potentially cross-reference individuals mentioned? or discard\n",
    "        elif self._is_mention(word):\n",
    "            word = self._process_mention(word)\n",
    "        # remove ascii characters \n",
    "\n",
    "        word = self._ascii_to_unicode(word)\n",
    "\n",
    "        word = re.sub(r'[^\\w\\s]',' ', word)\n",
    "#       word = self._word_stem(word)\n",
    "        if word in self.stop_words:\n",
    "            return ''\n",
    "        return word\n",
    "                \n",
    "    def _is_hashtag(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"#\"\n",
    "    \n",
    "    def _is_mention(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"@\"\n",
    "    \n",
    "    def _is_hyperlink(self, word):\n",
    "        return validators.url(word)\n",
    "    \n",
    "    def _process_hashtag(self, word):\n",
    "        return ''\n",
    "#         return word[1:]\n",
    "    \n",
    "    def _process_mention(self, word):\n",
    "        return ''\n",
    "#         return word[1:]\n",
    "    \n",
    "    def _ascii_to_unicode(self, word):\n",
    "        for uescape in re.findall(r'(\\\\u[0-9a-f]{4})', word):\n",
    "            try:\n",
    "                word = word.replace(uescape, '')  \n",
    "            except (UnicodeDecodeError, Exception):\n",
    "                print(\"Failed to decode: {}\".format(uescape))\n",
    "        return word\n",
    "    \n",
    "    def _word_stem(self, word):\n",
    "        return self.stemmer.stem(word)\n",
    "    \n",
    "    def save_model(self, name, model):\n",
    "        dir_path = path.join(getcwd(), \"models\", self.feature_selection_method)\n",
    "        model_fpath = path.join(dir_path, name + \".pkl\")\n",
    "        \n",
    "        if not path.exists(dir_path):\n",
    "            makedirs(dir_path)\n",
    "            \n",
    "        pickle.dump(model, open(model_fpath, 'wb'))\n",
    "        \n",
    "    def load_model(self, name):\n",
    "        dir_path = path.join(getcwd(), \"models\", self.feature_selection_method)\n",
    "        model_fpath = path.join(dir_path, name + \".pkl\")\n",
    "        \n",
    "        with open(model_fpath, 'rb') as model_pickle:\n",
    "            model = pickle.load(model_pickle)\n",
    "        return model\n",
    "\n",
    "class MetaClassifier:\n",
    "    def __init__(self, estimators, random_state):\n",
    "        self.estimators = estimators\n",
    "        self.encoder = OneHotEncoder()\n",
    "        self.base_classifier = LogisticRegression(random_state=random_state)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y_set = pd.DataFrame(index=X.index, columns=[item[0] for item in self.estimators])\n",
    "        \n",
    "        for name, classifier in self.estimators:\n",
    "            classifier.fit(X, y)\n",
    "            y_bar = classifier.predict(X)\n",
    "            y_set[name] = self.encoder.fit_transform(y_bar.reshape(-1, 1)).toarray()\n",
    "            \n",
    "        self.base_classifier.fit(y_set, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_set = pd.DataFrame(index=X.index, columns=[item[0] for item in self.estimators])\n",
    "\n",
    "        for name, classifier in self.estimators:\n",
    "            y_bar = classifier.predict(X)\n",
    "            y_set[name] = self.encoder.transform(y_bar.reshape(-1, 1)).toarray()\n",
    "                \n",
    "        return  self.base_classifier.predict(y_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steers', 'sparked', '07pm', 'anthonycarrello89', 'mnimo', 'rappel', 'chemist', 'pgfc', 'bundaburg', 'monoxide', 'sizi', 'linkedinlocal', 'shawshank', '1013mb', 'channy', 'hirdy', '3vs', 'cdf', 'bcom', 'bennell', 'gfe', 'termasuk', 'jisosu', 'paytrend', 'indis', 'aggregation', 'yanis', 'burglaries', 'snapfitnessredcliffe', 'djdjdjdjdjdijskskskks', 'af100w', 'belcher', 'streaker', 'bunkers', 'iiyakan', 'shue', 'sexxxxy', 'hurn', 'creido', 'toshiba', 'u6cdumaglo', 'flea', 'populations', 'skipers', 'durward', 'nasal', 'immobilize', 'waterman', 'aule', 'generates', 'escaping', 'crapshat', 'subjugation', 'desicion', 'flatness', 'sehat', 'codified', 'chup', 'crapchat', 'affogato', 'letterbox', 'll7g6r4kw3', 'weedit', 'hoola', 'ckaryorum', 'unnamed', 'torp', 'usbs', 'bahhon', 'xeff5arbwh', 'carded', 'busso', 'latifesib', 'shoesss', '8x', 'championing', 'miserably', 'in4th', 'erica', 'suaramu', 'triggering', 'guyssss', 'unhelpful', 'cannington', 'spits', 'deviated', 'copain', 'baeeeek', 'jtc', 'landsdale', 'meps', 'importanc', 'demdem', 'lessssgoooooo', 'netflixan', 'warga', 'glamsmiles', 'bz', 'zedddddddd', 'nickelodeon', 'meeeeee', 'conventions', 'karts', 'ricket', 'olle', 'tarka', '13gb', 'jisun', 'digs', 'pcs', 'iisang', 'todamax', 'bullyin', 'impar', 'unbot', 'sorrento_fc', 'myp', 'prefi', 'compfit', 'caduta', 'fordyce', 'abah', 'porcelain', 'domuz', 'hahahahahahahauayaa', 'poncy', 'croze', 'viti', 'renegades', 'cuteeee', 'otherside', 'cust', '715am', 'anjana', 'beachcomber', '012', 'toso', 'dogue', 'tendai', 'wrk', 'gestures', 'ditunggu', 'mending', 'soundgoodizer', 'ogfyfzxmea', 'tertentu', '011', 'satisfies', 'sadl', 'wolly', 'listeden', 'jogging', 'ieri', 'classrooms', 'becuase', 'pbeehiwv31', 'inkrat', 'wanneroo', 'conc', 'newbies', 'steindl', 'opport', 'aaa', 'wapo', 'hobley', 'tayla', 'attorn', 'punye', 'tritnya', 'ditempat', 'sympho', 'fn', 'gayyyyy', 'partied', 'wooooooo', 'blueberries', 'rhydz', 'morroco', 'extermin', 'wafl', 'zzzigarrr', 'mikayla', 'nuce', 'inaportante', 'fremantle', 'sbren', 'exaggerate', 'desa2', 'scaglie', 'carlisle', 'retir', 'souttu', 'pastikan', 'gregorio', 'kartway', 'rtnyypiw24', 'incluso', 'cryptowallet', 'liberates', 'hardys', 'cesaro', 'b4emifcvux', '80k', '84k', 'benchmarks', 'trie', 'memakainya', 'rog', 'sciency', 'mencoblos', 'aims', 'xc', 'popup', 'vcloud', 'selera', 'luchar', 'kalimot', 'byeee', 'bootcut', 'freos', 'launcher', 'crepe', 'cottesloe', 'sandgroper', 'contend', 'fjb2mqesdu', 'kathniels', 'deconstructing', 'batte', 'slammy', '0430294110', 'keambil', 'timur', 'septum', 'existent', 'lutang', 'probiotics', 'exocet', 'gituuu', 'paket', 'playof', 'realrock', 'heavan', 'pirini', 'ojie', 'turnstile', 'retracing', 'honne', 'longmire', 'penatnya', 'fuqwpptbo3', '180rb', 'soriii', 'nadal', '0477', 'sorr', 'chevron', 'lemmy', 'nelpon', 'hahahahahahahah', '637', 'hawi', 'scuffs', 'explicit', 'flaca', 'messiah', 'glamsmile', 'liker', 'ohya', '6akjqvlvgk', '014', 'loony', 'ahvalde', 'lymphedema', 'wipes', 'deadest', 'compressors', '3vs1', 'wnpvdvmemk', 'us11', 'walan', 'sheppard', 'dal', 'crostata', 'bidirectional', 'lbaoibhwbh', 'dayssssssssss', 'tahni', '477', 'stata', 'gambier', 'okudum', '20wk', 'paslon', 'cmmnnnnn', 'accessorising', 'heartedly', 'externalising', 'mz3', 'assisi', 'yasmin', 'liberalism', 'turnip', 'zola', 'kellyy', 'ankur', 'congraatss', 'posisinya', 'entr', 'splm', 'giver', 'wwii', 'tourniquet', '1x04', 'embezzlement', 'celine', 'bhaseen', 'redistribute', 'gooopoko', 'soiled', 'shaa', 'coexist', 'unload', 'saipay', 'brewi', 'khalesi', 'meditatt', 'xrs4xpkavy', 'sepulveda', 'adapting', 'pda', 'leederville', 'snapshat', 'wyvren', 'mothsfkn', 'sparingly', 'fucurry', 'mzungu', 'desolate', 'complicant', 'gethsemane', 'drogo', 'identifi', 'bestnya', 'enthusiast', 'minimizing', 'dodzie', 'koenji', '273k', 'nailbiting', 'belmontforum', 'ricochets', 'klase', 'kayla', 'simco', '9bfhu8tgyf', 'manuod', 'bb8', 'changbin', 'offshore', 'lazing', 'misse', 'kasus', 'ammo', 'geel', 'amok', 'thickened', 'cioccolato', 'existencia', 'hillview', 'matso', 'gorkhpur', 'treefrogger', 'slayyy', 'bocelli', 'eithe', 'maximus', 'gada', 'sabajap', 'igad', 'dissection', 'ncp', 'frustratingly', 'englishman', 'electability', 'vanities', 'jaker', 'dojo', 'politik', 'lessssssssssleyyuyuu', 'theglobeperth', 'siparii', 'shading', 'lymph', 'silverware', 'carlin', 'godont', 'kaburgas', 'johanson', 'annually'}\n",
      "Time taken: 0:01:35.092273\n"
     ]
    }
   ],
   "source": [
    "# classifier_set = [\"LinearSVM\"]\n",
    "classifier_set = [\"One-R\"]\n",
    "voting_strategy = \"meta_classification\"\n",
    "combine_classifiers = False\n",
    "\n",
    "gt = GeoTagger(\n",
    "    inputs = inputs,\n",
    "    target = output,\n",
    "    classifier_set = classifier_set,\n",
    "    ensemble_strategy = voting_strategy,\n",
    "    feature_selection_method = \"word_locality_heuristic\",\n",
    "    seed = 500,\n",
    "    combine_classifiers = combine_classifiers\n",
    ")\n",
    "\n",
    "start = datetime.now()\n",
    "gt.train(x_train, y_train)\n",
    "end = datetime.now()\n",
    "print(\"Time taken: {}\".format(end - start))\n",
    "\n",
    "# gt2 = GeoTagger(\n",
    "#     inputs = inputs,\n",
    "#     target = output,\n",
    "#     classifier_set = classifier_set,\n",
    "#     ensemble_strategy = voting_strategy,\n",
    "#     feature_selection_method = \"baseline_10\",\n",
    "#     seed = 500,\n",
    "#     combine_classifiers = combine_classifiers\n",
    "# )\n",
    "\n",
    "# start = datetime.now()\n",
    "# gt2.train(x_train, y_train)\n",
    "# end = datetime.now()\n",
    "# print(\"Time taken: {}\".format(end - start))\n",
    "\n",
    "# gt3 = GeoTagger(\n",
    "#     inputs = inputs,\n",
    "#     target = output,\n",
    "#     classifier_set = classifier_set,\n",
    "#     ensemble_strategy = voting_strategy,\n",
    "#     feature_selection_method = \"baseline_50\",\n",
    "#     seed = 500,\n",
    "#     combine_classifiers = combine_classifiers\n",
    "# )\n",
    "\n",
    "# start = datetime.now()\n",
    "# gt3.train(x_train, y_train)\n",
    "# end = datetime.now()\n",
    "# print(\"Time taken: {}\".format(end - start))\n",
    "\n",
    "# gt4 = GeoTagger(\n",
    "#     inputs = inputs,\n",
    "#     target = output,\n",
    "#     classifier_set = classifier_set,\n",
    "#     ensemble_strategy = voting_strategy,\n",
    "#     feature_selection_method = \"baseline_100\",\n",
    "#     seed = 500,\n",
    "#     combine_classifiers = combine_classifiers\n",
    "# )\n",
    "\n",
    "# start = datetime.now()\n",
    "# gt4.train(x_train, y_train)\n",
    "# end = datetime.now()\n",
    "# print(\"Time taken: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-ca4dc6a160fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#predict the class labels of a set of test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mybars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# ybars2 = gt2.predict(x_dev)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(\"Done\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-a09abe45505e>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, load_model)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-a09abe45505e>\u001b[0m in \u001b[0;36mbase_predict\u001b[0;34m(self, x, load_model)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mclassifier_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0my_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier_prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "#predict the class labels of a set of test data\n",
    "ybars = gt.predict(x_dev, load_model=True)\n",
    "print(\"Done\")\n",
    "# ybars2 = gt2.predict(x_dev)\n",
    "# print(\"Done\")\n",
    "# ybars3 = gt3.predict(x_dev)\n",
    "# print(\"Done\")\n",
    "# ybars4 = gt4.predict(x_dev)\n",
    "# print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [gt, gt2, gt3, gt4]\n",
    "# predictions = [ybars, ybars2, ybars3, ybars4]\n",
    "# evals = [\"accuracy\", \"precision_recall_f-score_with_micro\"]\n",
    "\n",
    "# for model, prediction in zip(models, predictions):\n",
    "#     for method in evals:\n",
    "#         report, confusion = model.evaluate(prediction, y_dev, method)\n",
    "#         print(\"{}: {}\".format(method, report, ))\n",
    "accScores = gt.evaluate(ybars, y_dev)\n",
    "# otherScores = gt.evaluate(ybars, y_dev, \"precision_recall_f-score_with_micro\")\n",
    "# accScores2 = gt2.evaluate(ybars2, y_dev)\n",
    "# otherScores2 = gt2.evaluate(ybars2, y_dev, \"precision_recall_f-score_with_micro\")\n",
    "# accScores3 = gt3.evaluate(ybars3, y_dev)\n",
    "# otherScores3 = gt3.evaluate(ybars3, y_dev, \"precision_recall_f-score_with_micro\")\n",
    "# accScores4 = gt4.evaluate(ybars4, y_dev)\n",
    "# otherScores4 = gt4.evaluate(ybars4, y_dev, \"precision_recall_f-score_with_micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accScores) #, accScores2, accScores3, accScores4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(np.unique(ybars, return_counts=True))\n",
    "# print(np.unique(ybars2, return_counts=True))\n",
    "# print(np.unique(ybars3, return_counts=True))\n",
    "# print(np.unique(ybars4, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Resources\n",
    "- https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
