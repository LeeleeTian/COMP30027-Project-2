{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Build Baseline Location Classifier\\nApproaches:\\n- instance representation in form of 'bag' of words \\n    - features: word frequencies, metadata \\n    - exclude rare words in data set (used by less than 3 users )\\n- gramatical structure with NLP\\n- model instances in terms of authors instead of documents \\n\\n\\n- Baseline Classifier: Naive Bayes Model ()\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Build Baseline Location Classifier\n",
    "Approaches:\n",
    "- instance representation in form of 'bag' of words \n",
    "    - features: word frequencies, metadata \n",
    "    - exclude rare words in data set (used by less than 3 users )\n",
    "- gramatical structure with NLP\n",
    "- model instances in terms of authors instead of documents \n",
    "\n",
    "\n",
    "- Baseline Classifier: Naive Bayes Model ()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from os import path, getcwd\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re, string\n",
    "import validators\n",
    "from info_gain.info_gain import info_gain_ratio\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# nltk.download('english')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir = path.join(getcwd(), \"2019S1-proj2-datah\")\n",
    "\n",
    "train_data = \"train-raw.tsv\"\n",
    "test_data = \"test-raw.tsv\"\n",
    "dev_data = \"dev-raw.tsv\"\n",
    "\n",
    "train_fpath = path.join(fdir, train_data)\n",
    "test_fpath = path.join(fdir, test_data)\n",
    "dev_fpath = path.join(fdir, dev_data)\n",
    "\n",
    "train = pd.read_csv(train_fpath, encoding=\"utf_8\", delimiter=\"\\t\")\n",
    "test = pd.read_csv(test_fpath, encoding=\"utf_8\", delimiter=\"\\t\")\n",
    "dev = pd.read_csv(dev_fpath, encoding=\"utf_8\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'Text'\n",
    "output = 'Location'\n",
    "x_train = train[inputs]\n",
    "y_train = train[output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTagger:\n",
    "    _FEATURE_SELECTION = [\"baseline_10\", \"baseline_50\", \"baseline_100\", \"info_gain_ratio\", \"word_locality_heuristic\", \"tf_idf\"]\n",
    "    _VOTING_STRATEGY = [\"simple_voting\", \"bagging\", \"stacking\", \"random_forest\", \"boosting\"]\n",
    "    _CLASSIFIERS = [\"Zero-R\", \"One-R\", \"Decision-Tree\", \"MultinomialNB\", \"LinearSVM\", \"SemiSupervised\"]\n",
    "    _EVALUATION_METRIC = [\"accuracy\", \"precision_recall_f-score_with_macro\", \"precision_recall_f-score_with_micro\"]\n",
    "    \n",
    "    def __init__(self, inputs, target, classifier_set=[\"MultinomialNB\"], voting_strategy=\"simple_voting\", feature_selection_method=\"baseline_100\", seed=500):\n",
    "        self.inputs = inputs\n",
    "        self.target = target\n",
    "        self.classifier_set = classifier_set\n",
    "        self.voting_strategy = voting_strategy\n",
    "        self.feature_selection_method = feature_selection_method\n",
    "        self.classifier_set = self._combine_classifier_set(classifier_set)\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        trains a classifier given the training data and their corresponding class labels\n",
    "        \"\"\"\n",
    "        self.classes = y.unique()\n",
    "        X = self.preprocess(X, y, train=True)\n",
    "        \n",
    "        for classifier in self.classifier_set.values():\n",
    "            classifier.fit(X, y)\n",
    "          \n",
    "    def predict(self, X):\n",
    "        x = self.preprocess(X)\n",
    "        predictions = pd.DataFrame()\n",
    "                \n",
    "        for name, classifier in self.classifier_set.items():\n",
    "            classifier_prediction = classifier.predict(x, y)\n",
    "            predictions[name] = classifier_predictions\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, ybar, y, metric):\n",
    "        #TODO: eval method\n",
    "        if not metric in GeoTagger._EVALUATION_METRIC:\n",
    "                print(\"Invalid Evaluation Metric: {}. Choose one of \\\n",
    "                ({})\".format(metric, \", \".join(GeoTagger._EVALUATION_METRIC))) \n",
    "                return\n",
    "            \n",
    "        if metric == \"accuracy\":\n",
    "            score = accuracy_score(ybar, y)\n",
    "        if metric == \"precision\":\n",
    "            score = precision_recall_fscore_support(y_true, y_pred, average='macro'),\n",
    "        if metric == \"precision_recall_f-score_with_micro\":\n",
    "            score = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "            \n",
    "        return score\n",
    "    \n",
    "    def preprocess(self, X, y=None, train=False):\n",
    "        \"\"\"\n",
    "         - Filter rare words (urls, typos rare names, punctuation symbols)\n",
    "         - calculate word frequencies \n",
    "         - metadata\n",
    "        \"\"\"\n",
    "        X = self.filter(X)\n",
    "\n",
    "        if train:\n",
    "            self.feature_selection(X, y)\n",
    "        \n",
    "        X = self.bag_of_words(X)\n",
    "            \n",
    "        return X \n",
    "    \n",
    "    def bag_of_words(self, X):\n",
    "        print(X)\n",
    "        _x = pd.DataFrame(\n",
    "            x.apply(lambda text: word in sorted(list(self.features) for word in text.split())),\n",
    "            index=X.index, \n",
    "            columns=self.features\n",
    "        )\n",
    "        print(_x)\n",
    "    \n",
    "    def _combine_classifier_set(self, classifiers):\n",
    "        classifier_set = defaultdict()\n",
    "        \n",
    "        for classifier in classifiers:\n",
    "            if not classifier in GeoTagger._CLASSIFIERS:\n",
    "                print(\"Invalid Classifier: {}. Choose one of \\\n",
    "                ({})\".format(classifier, \", \".join(GeoTagger._CLASSIFIERS)))\n",
    "                continue\n",
    "                \n",
    "            if classifier == \"Zero-R\":\n",
    "                classifier_set[classifier] = DummyClassifier(strategy='most_frequent', random_state=self.seed)\n",
    "            elif classifier == \"One-R\":\n",
    "                classifier_set[classifier] = DecisionTreeClassifier(max_depth=1, criterion=\"entropy\", random_state=self.seed)\n",
    "            elif classifier == \"Decision-Tree\":\n",
    "                classifier_set[classifier] = DecisionTreeClassifier(max_depth=None, criterion=\"entropy\", random_state=self.seed)\n",
    "            elif classifier == \"MultinomialNB\":\n",
    "                classifier_set[classifier] = MultinomialNB()\n",
    "            elif classifier == \"LinearSVM\":\n",
    "                continue\n",
    "            elif classifier == \"SemiSupervised\":\n",
    "                continue\n",
    "\n",
    "        return classifier_set\n",
    "                \n",
    "    def feature_selection(self, X, y):\n",
    "        \"\"\"\n",
    "        (1) Information Gain Ratio (IGR) - across all states S, is \n",
    "            defined as the ratio between its information gain value IG, \n",
    "            which measures the decrease in class entropy H that w brings,\n",
    "            and its intrinsic entropy IV, which measures the entropy of \n",
    "            the presence versus the absence of that word\n",
    "            \n",
    "        (2) Word Locality Heuristic (WLH) - promotes words primarily \n",
    "            associated with one location. measure the probability of \n",
    "            a word occurring in a state, divided by its probability to \n",
    "            appear in any state. Then, for a given word w, we define the \n",
    "            WLH as the maximum such probability across all the states S\n",
    "        \"\"\"\n",
    "        if self.feature_selection_method not in GeoTagger._FEATURE_SELECTION:\n",
    "            print(\"Invalid Feature Selection method: {}. Choose one of \\\n",
    "            ({})\".format(self.feature_selection_method, \", \".join(GeoTagger._FEATURE_SELECTION)))\n",
    "            return \n",
    "        \n",
    "        if self.feature_selection_method == \"baseline_10\":\n",
    "            self.baseline_heuristic(X, y, \"10\")\n",
    "        elif self.feature_selection_method == \"baseline_50\":\n",
    "            self.baseline_heuristic(X, y, \"50\")\n",
    "        elif self.feature_selection_method == \"baseline_100\":\n",
    "            self.baseline_heuristic(X, y, \"100\")\n",
    "        elif self.feature_selection_method == \"info_gain_ratio\":\n",
    "#             self.information_gain_ratio(x)\n",
    "            return\n",
    "        elif self.feature_selection_method == \"word_locality_heuristic\":\n",
    "            self.word_locality_weight(X, y)\n",
    "        elif self.feature_selection_method == \"tf_idf\":\n",
    "            self.tf_idf(X, y)\n",
    "\n",
    "    def baseline_heuristic(self, X, y, top_n):\n",
    "#         fdir = path.join(getcwd(), \"2019S1-proj2-datah\")\n",
    "        feature_fpath = path.join(fdir, \"train-top\" + top_n + \".csv\")\n",
    "        \n",
    "        if not path.exists(feature_fpath):\n",
    "            print(\"Baseline Heuristic path {} does not exist\".format(feature_fpath))\n",
    "            return\n",
    "        \n",
    "        features = open(feature_fpath).readline()\n",
    "        features = features.split(\",\")\n",
    "        features.remove(\"Instance_ID\")\n",
    "        features.remove(\"Location\\n\")\n",
    "        self.features = set(features)\n",
    "\n",
    "    def word_locality_weight(self, X, y):\n",
    "        \"\"\"\n",
    "        calculate frequencies of data \n",
    "        Measure frequency and divide by sum of freqencies of all words\n",
    "        \"\"\"\n",
    "        locations = self.classes + ['Total',]\n",
    "        word_location_weight = {label: defaultdict() for label in locations}\n",
    "        \n",
    "        for x_i, y_i in zip(X.index, y.index):\n",
    "            text = X.loc[x_i].split()\n",
    "            for word in text:\n",
    "                word_location_weight[y.loc[y_i]][word] += 1\n",
    "                word_location_weight[y.loc[y_i]]['Total'] += 1\n",
    "                word_location_weight['Total'][word] += 1\n",
    "                word_location_weight['Total']['Total'] += 1\n",
    "                \n",
    "    \n",
    "    def information_gain_ratio(self, x):\n",
    "#         return info_gain_ratio\n",
    "        pass\n",
    "    \n",
    "    def tf_idf(self, X, y):\n",
    "        vectorizer = TfidfVectorizer(stop_words=self.stop_words, max_features=400)\n",
    "\n",
    "        location_word_list = {label: '' for label in self.classes}\n",
    "\n",
    "        for x_i, y_i in zip(X.index, y.index):\n",
    "            location_word_list[y.loc[y_i]] += X.loc[x_i] + \" \"\n",
    "        \n",
    "        labels = location_word_list.keys()\n",
    "        corpus = location_word_list.values()\n",
    "        vectorizer.fit(corpus, labels)\n",
    "        self.features = set(vectorizer.get_feature_names())\n",
    "        \n",
    "    def filter(self, x):\n",
    "        return x.apply(self.filter_text)\n",
    "    \n",
    "    def filter_text(self, text):\n",
    "        return ' '.join(self.filter_word(w) for w in text.split())\n",
    "        \n",
    "    def filter_word(self, word):\n",
    "        word = word.lower()\n",
    "        # extract keywords from hashtag \n",
    "        if self._is_hyperlink(word):\n",
    "            return ''\n",
    "        elif self._is_hashtag(word):\n",
    "            word = self._process_hashtag(word)\n",
    "        # potentially cross-reference individuals mentioned? or discard\n",
    "        elif self._is_mention(word):\n",
    "            word = self._process_mention(word)\n",
    "        # remove ascii characters \n",
    "        else:\n",
    "            word = self._ascii_to_unicode(word)\n",
    "            word = self._word_stem(word)\n",
    "            word = re.sub(r'[^\\w\\s]',' ', word)\n",
    "        return word\n",
    "                \n",
    "    def _is_hashtag(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"#\"\n",
    "    \n",
    "    def _is_mention(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"@\"\n",
    "    \n",
    "    def _is_hyperlink(self, word):\n",
    "        return validators.url(word)\n",
    "    \n",
    "    def _process_hashtag(self, word):\n",
    "        return word[1:]\n",
    "    \n",
    "    def _process_mention(self, word):\n",
    "        return word[1:]\n",
    "    \n",
    "    def _ascii_to_unicode(self, word):\n",
    "        for uescape in re.findall(r'(\\\\u[0-9a-f]{4})', word):\n",
    "            try:\n",
    "#                 print(uescape.encode('utf-8').decode('unicode-escape'), type(uescape.encode('utf-8').decode('unicode-escape')))\n",
    "#                 word = re.sub(uescape, uescape.encode('utf-8'), word)\n",
    "#                 print(word)\n",
    "#                 print(uescape, type(uescape))\n",
    "                word = word.replace(uescape, '')  \n",
    "            except (UnicodeDecodeError, Exception):\n",
    "                print(\"Failed to decode: {}\".format(uescape))\n",
    "        return word\n",
    "    \n",
    "    def _word_stem(self, word):\n",
    "        return self.stemmer.stem(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                           melbourne  victoria  australia \n",
      "1         theage of cours it cost more  minimum standard...\n",
      "2         hope peopl make just as much nois as they did ...\n",
      "3         pour the perfect prosecco    the globe  perth ...\n",
      "4          lni lose traction at 0 014  see this retrac b...\n",
      "5                                                          \n",
      "6                             ashleighjayy_ me  this bitch \n",
      "7         anaolfan i  you   i could never be offend by y...\n",
      "8         ihatejoelkim welcom to australia  hope you hav...\n",
      "9         mea evolv confer markbouris session  i learnt ...\n",
      "10                          first tast of winter in autumn \n",
      "11                                                         \n",
      "12        selfiesunday lost tabby  amp  white caramel ca...\n",
      "13         artsdesire give her credit  she look incredible \n",
      "14        pringster78 chadderbox2018 i kind of do the op...\n",
      "15                           gramercypark it great isnt it \n",
      "16                               i date her  not mj xoxoxo \n",
      "17        aussieandycx imscaredq deadest kane would dead...\n",
      "18        if i see got spoiler  and esp not tag or anyth...\n",
      "19                                                         \n",
      "20                   pleaseuseaussie rods108443078 hope so \n",
      "21                        x2 babe  biggest scam i tell you \n",
      "22        idgimaddy exoglobal b_hundred_hyun 19920506b4 ...\n",
      "23                                              sbren sbeve\n",
      "24        grassy  bitter  kiwi  hops  clean   drink a pa...\n",
      "25        fatherbob father bob  thankyou i went to an ex...\n",
      "26        30th birthday dinner  thank snapsbykatie   gre...\n",
      "27        clementine_ford us fella were all drawn to it ...\n",
      "28                                                         \n",
      "29        koko bunny  the easter bunni arriv at my door ...\n",
      "                                ...                        \n",
      "103330    wow  im not veri good at get the pic the right...\n",
      "103331    i gather the govt want to further clog up punt...\n",
      "103332    visual is the key to success  dream big  then ...\n",
      "103333    countri ive been to  fiji  australia  south ko...\n",
      "103334    taodehaas carolemorrissey yes i agree     some...\n",
      "103335                                         wiley_health\n",
      "103336                                   unbeliev riflepete\n",
      "103337    thank thejosephnaim for come to see jerseyboys...\n",
      "103338    harrytrevor8888 blondebonnie94 it is incompat ...\n",
      "103339    the day befor the budget  sportsbet had odd of...\n",
      "103340    sophie_walsh9 where was this sophi   he was in...\n",
      "103341    peopl who are not scientist but who question t...\n",
      "103342    my mom came up to visit this morn and to run a...\n",
      "103343                                                ts7  \n",
      "103344    iamtheoracle i know  i know  those peopl are c...\n",
      "103345    stephanieloveuk alreadi seen enough there no n...\n",
      "103346                               colinhussey22 gorillaz\n",
      "103347    insufferablscot tinyelbows_ part of me think y...\n",
      "103348                               cdelafuentez lol exact\n",
      "103349    rmharmon2004 outsid of the context ofwrestl tw...\n",
      "103350                     kirstywebeck so smash it kirsty \n",
      "103351                                                     \n",
      "103352    tbdalton5 that they are  i have had 2 in my li...\n",
      "103353    grayconnolly gemmatognini elizajbarr your get ...\n",
      "103354    australian izzyfolau veri interest that you ge...\n",
      "103355                    solzinho bom pra ir na praia hoje\n",
      "103356    suzdavies13 of me    thank you  sure you dont ...\n",
      "103357    gracespelman have you heard of dune  seen some...\n",
      "103358        id bang you harder than an old chevrolet door\n",
      "103359     mezmfield chief_comanche utopiana fraser_anning \n",
      "Name: Text, Length: 103360, dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (41344000, 1), indices imply (103360, 400)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1650\u001b[0m                 blocks = [make_block(values=blocks[0],\n\u001b[0;32m-> 1651\u001b[0;31m                                      placement=slice(0, len(axes[0])))]\n\u001b[0m\u001b[1;32m   1652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   3094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3095\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;34m'Wrong number of items passed {val}, placement implies '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                 '{mgr}'.format(val=len(self.values), mgr=len(self.mgr_locs)))\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Wrong number of items passed 1, placement implies 400",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-240-ecc278388685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mx_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-239-74ea66fa2845>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \"\"\"\n\u001b[1;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-239-74ea66fa2845>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(self, X, y, train)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbag_of_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-239-74ea66fa2845>\u001b[0m in \u001b[0;36mbag_of_words\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         )\n\u001b[1;32m     76\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    449\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m                     mgr = init_ndarray(data, index, columns, dtype=dtype,\n\u001b[0;32m--> 451\u001b[0;31m                                        copy=copy)\n\u001b[0m\u001b[1;32m    452\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[0;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_infer_to_datetimelike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[0;34m(blocks, axes)\u001b[0m\n\u001b[1;32m   1658\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'values'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1660\u001b[0;31m         \u001b[0mconstruction_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1661\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[0;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m     raise ValueError(\"Shape of passed values is {0}, indices imply {1}\".format(\n\u001b[0;32m-> 1691\u001b[0;31m         passed, implied))\n\u001b[0m\u001b[1;32m   1692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shape of passed values is (41344000, 1), indices imply (103360, 400)"
     ]
    }
   ],
   "source": [
    "gt = GeoTagger(\n",
    "    inputs = inputs,\n",
    "    target = output,\n",
    "    classifier_set = [\"MultinomialNB\"], \n",
    "    voting_strategy = \"simple_voting\",\n",
    "    feature_selection_method = \"tf_idf\"\n",
    ")\n",
    "\n",
    "x_features = gt.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the class labels of a set of test data\n",
    "# ybar = self.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate classifier performance\n",
    "# score = self.evaluate(ybar, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Resources\n",
    "- https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
