{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string\n",
    "from os import path, getcwd\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier, BaggingClassifier, RandomForestClassifier\n",
    "\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import validators\n",
    "import nltk\n",
    "from info_gain.info_gain import info_gain_ratio\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir = path.join(getcwd(), \"2019S1-proj2-datah\")\n",
    "\n",
    "train_data = \"train-raw.tsv\"\n",
    "test_data = \"test-raw.tsv\"\n",
    "dev_data = \"dev-raw.tsv\"\n",
    "\n",
    "train_fpath = path.join(fdir, train_data)\n",
    "test_fpath = path.join(fdir, test_data)\n",
    "dev_fpath = path.join(fdir, dev_data)\n",
    "\n",
    "train = pd.read_csv(train_fpath, encoding=\"utf_8\", delimiter=\"\\t\", index_col=\"Instance_ID\")\n",
    "test = pd.read_csv(test_fpath, encoding=\"utf_8\", delimiter=\"\\t\" , index_col=\"Instance_ID\")\n",
    "dev = pd.read_csv(dev_fpath, encoding=\"utf_8\", delimiter=\"\\t\" , index_col=\"Instance_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'Text'\n",
    "output = 'Location'\n",
    "x_train = train[inputs]\n",
    "y_train = train[output]\n",
    "x_test = test[inputs]\n",
    "y_test = test[output]\n",
    "x_dev = dev[inputs]\n",
    "y_dev = dev[output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTagger:\n",
    "    _FEATURE_SELECTION = [\"baseline_10\", \"baseline_50\", \"baseline_100\", \"info_gain_ratio\", \"word_locality_heuristic\", \"tf_idf\"]\n",
    "    _ENSEMBLE_STRATEGY = [\"simple_voting\", \"meta_classification\", \"bagging\", \"random_forest\", \"boosting\"]\n",
    "    _CLASSIFIERS = [\"Zero-R\", \"One-R\", \"Decision-Tree\", \"MultinomialNB\", \"LinearSVM\", \"SemiSupervised\"]\n",
    "    _EVALUATION_METRIC = [\"accuracy\", \"precision_recall_f-score_with_macro\", \"precision_recall_f-score_with_micro\"]\n",
    "    \n",
    "    def __init__(self, inputs, target, classifier_set=[\"MultinomialNB\"], ensemble_strategy=\"simple_voting\", feature_selection_method=\"baseline_100\", seed=500, combine_classifiers=False, n_features=400):\n",
    "        self.inputs = inputs\n",
    "        self.target = target\n",
    "        self.classifier_set = classifier_set\n",
    "        self.ensemble_strategy = ensemble_strategy\n",
    "        self.feature_selection_method = feature_selection_method\n",
    "        self.seed = seed\n",
    "        np.random.seed(seed)\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.combine_classifiers = combine_classifiers\n",
    "        self.classifier_set = self._generate_classifier_set(classifier_set)\n",
    "        self.combined_classifier = None if not self.combine_classifiers else self.generate_ensemble_classifier()\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        trains a classifier given the training data and their corresponding class labels\n",
    "        \"\"\"\n",
    "        self.classes = y.unique()\n",
    "        X = self.preprocess(X, y, train=True)\n",
    "        \n",
    "        if self.combine_classifiers:\n",
    "            self.combined_classifier.fit(X, y)\n",
    "        else:\n",
    "            for classifier in self.classifier_set.values():\n",
    "                classifier.fit(X, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predicts a set of classifiers given some development data\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        \n",
    "        if self.combine_classifiers:\n",
    "            return self.combined_classifier.predict(X)\n",
    "        else:\n",
    "            y_set = pd.DataFrame()\n",
    "\n",
    "            for name, classifier in self.classifier_set.items():\n",
    "                classifier_prediction = classifier.predict(X)\n",
    "                y_set[name] = classifier_prediction\n",
    "                \n",
    "        return y_set\n",
    "        \n",
    "    \n",
    "    def evaluate(self, ybar, y):\n",
    "        \"\"\"\n",
    "        evaluates a class' predictions given the correct class labels and an evaluation metric\n",
    "        \"\"\"\n",
    "#         if not metric in GeoTagger._EVALUATION_METRIC:\n",
    "#             print(\"Invalid Evaluation Metric: {}. Choose one of \\\n",
    "#             ({})\".format(metric, \", \".join(GeoTagger._EVALUATION_METRIC)))\n",
    "#             return\n",
    "            \n",
    "        score_set = defaultdict()\n",
    "        \n",
    "        if self.combine_classifiers:\n",
    "            classifiers = [self.ensemble_strategy, ]\n",
    "            ybar = pd.DataFrame(ybar, columns=classifiers, index = y.index)\n",
    "        else:\n",
    "            classifiers = self.classifier_set\n",
    "\n",
    "        for name, y_pred in ybar.items():\n",
    "            accuracy = accuracy_score(y, y_pred)\n",
    "            score_set[name] = accuracy \n",
    "#             report = classification_report(ybar, y, self.target)\n",
    "#             confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "\n",
    "        return score_set\n",
    "    \n",
    "    def cross_validation(self, X, y, metric):\n",
    "        score_set = defaultdict()\n",
    "        \n",
    "        X = self.preprocess(X)\n",
    "\n",
    "        if self.combine_classifiers:\n",
    "            classifiers = [self.ensemble_strategy, ]\n",
    "            ybar = pd.DataFrame(ybar, columns=classifiers, index = y.index)\n",
    "        else:\n",
    "            classifiers = self.classifier_set\n",
    "\n",
    "        for classifier in classifiers:\n",
    "            score_set[metric] = cross_validate(classifier, X, y, cv=10)\n",
    "        return score_set\n",
    "    \n",
    "    \n",
    "    def preprocess(self, X, y=None, train=False):\n",
    "        \"\"\"\n",
    "         - Filter rare words (urls, typos rare names, punctuation symbols)\n",
    "         - calculate word frequencies \n",
    "         - metadata\n",
    "        \"\"\"\n",
    "        X = self.filter(X)\n",
    "\n",
    "        if train:\n",
    "            self.feature_selection(X, y)\n",
    "        \n",
    "        X = self.bag_of_words(X)\n",
    "            \n",
    "        return X \n",
    "    \n",
    "    def bag_of_words(self, X):\n",
    "        _x = pd.DataFrame(\n",
    "            [[(word in text) for word in sorted(list(self.features))] for text in X.values],\n",
    "            index=X.index, \n",
    "            columns=self.features,\n",
    "            dtype=np.uint8\n",
    "        )        \n",
    "        return _x\n",
    "    \n",
    "    def generate_ensemble_classifier(self):\n",
    "        if not self.combine_classifiers:\n",
    "            return None\n",
    "        \n",
    "        if not self.ensemble_strategy in GeoTagger._ENSEMBLE_STRATEGY:\n",
    "            print(\"Invalid Ensemble Strategy Metric: {}. Choose one of \\\n",
    "            ({})\".format(metric, \", \".join(GeoTagger._ENSEMBLE_STRATEGY)))\n",
    "            return None\n",
    "        \n",
    "        if self.ensemble_strategy == \"simple_voting\":\n",
    "            combined_classifier = VotingClassifier(self.classifier_set.items(), 'hard')\n",
    "        elif self.ensemble_strategy == \"meta_classification\":\n",
    "            combined_classifier = MetaClassifier(self.classifier_set.items(), self.seed)\n",
    "        elif self.ensemble_strategy == \"bagging\":\n",
    "            base_classifier = DecisionTreeClassifier(max_features=None, max_leaf_nodes=999)\n",
    "            combined_classifier = BaggingClassifier(base_estimator=base_classifier, max_features=self.n_features, random_state=self.seed)\n",
    "        elif self.ensemble_strategy == \"random_forest\":\n",
    "            combined_classifier = RandomForestClassifier()\n",
    "        elif self.ensemble_strategy == \"boosting\":\n",
    "            combined_classifier = GradientBoostingClassifier()\n",
    "        \n",
    "        return combined_classifier\n",
    "        \n",
    "    \n",
    "    def _generate_classifier_set(self, classifiers):\n",
    "        classifier_set = defaultdict()\n",
    "        \n",
    "        for classifier in classifiers:\n",
    "            if not classifier in GeoTagger._CLASSIFIERS:\n",
    "                print(\"Invalid Classifier: {}. Choose one of \\\n",
    "                ({})\".format(classifier, \", \".join(GeoTagger._CLASSIFIERS)))\n",
    "                continue\n",
    "                \n",
    "            if classifier == \"Zero-R\":\n",
    "                classifier_set[classifier] = DummyClassifier(strategy='most_frequent', random_state=self.seed)\n",
    "            elif classifier == \"One-R\":\n",
    "                classifier_set[classifier] = DecisionTreeClassifier(max_depth=1, criterion=\"entropy\", random_state=self.seed)\n",
    "            elif classifier == \"Decision-Tree\":\n",
    "                classifier_set[classifier] = DecisionTreeClassifier(max_depth=None, criterion=\"entropy\", random_state=self.seed)\n",
    "            elif classifier == \"MultinomialNB\":\n",
    "                classifier_set[classifier] = MultinomialNB()\n",
    "            elif classifier == \"LinearSVM\":\n",
    "                classifier_set[classifier] = svm.LinearSVC(random_state=self.seed)\n",
    "            elif classifier == \"SemiSupervised\":\n",
    "                classifier_set[classifier] = LabelSpreading(kernel=\"knn\", n_neighbors=7, alpha=0.2)\n",
    "        return classifier_set\n",
    "                \n",
    "    def feature_selection(self, X, y):\n",
    "        \"\"\"\n",
    "        (1) Information Gain Ratio (IGR) - across all states S, is \n",
    "            defined as the ratio between its information gain value IG, \n",
    "            which measures the decrease in class entropy H that w brings,\n",
    "            and its intrinsic entropy IV, which measures the entropy of \n",
    "            the presence versus the absence of that word\n",
    "            \n",
    "        (2) Word Locality Heuristic (WLH) - promotes words primarily \n",
    "            associated with one location. measure the probability of \n",
    "            a word occurring in a state, divided by its probability to \n",
    "            appear in any state. Then, for a given word w, we define the \n",
    "            WLH as the maximum such probability across all the states S\n",
    "        \"\"\"\n",
    "        if self.feature_selection_method not in GeoTagger._FEATURE_SELECTION:\n",
    "            print(\"Invalid Feature Selection method: {}. Choose one of \\\n",
    "            ({})\".format(self.feature_selection_method, \", \".join(GeoTagger._FEATURE_SELECTION)))\n",
    "            return \n",
    "        \n",
    "        if self.feature_selection_method == \"baseline_10\":\n",
    "            self.baseline_heuristic(X, y, \"10\")\n",
    "        elif self.feature_selection_method == \"baseline_50\":\n",
    "            self.baseline_heuristic(X, y, \"50\")\n",
    "        elif self.feature_selection_method == \"baseline_100\":\n",
    "            self.baseline_heuristic(X, y, \"100\")\n",
    "        elif self.feature_selection_method == \"info_gain_ratio\":\n",
    "#             self.information_gain_ratio(x)\n",
    "            return\n",
    "        elif self.feature_selection_method == \"word_locality_heuristic\":\n",
    "            self.word_locality_weight(X, y)\n",
    "        elif self.feature_selection_method == \"tf_idf\":\n",
    "            self.term_frequency_inverse_city_frequency(X, y)\n",
    "\n",
    "    def baseline_heuristic(self, X, y, top_n):\n",
    "        feature_fpath = path.join(fdir, \"train-top\" + top_n + \".csv\")\n",
    "        \n",
    "        if not path.exists(feature_fpath):\n",
    "            print(\"Baseline Heuristic path {} does not exist\".format(feature_fpath))\n",
    "            return\n",
    "        \n",
    "        features = open(feature_fpath).readline()\n",
    "        features = features.split(\",\")\n",
    "        features.remove(\"Instance_ID\")\n",
    "        features.remove(\"Location\\n\")\n",
    "        self.features = set(features)\n",
    "\n",
    "    def word_locality_weight(self, X, y):\n",
    "        \"\"\"\n",
    "        calculate frequencies of data \n",
    "        Measure frequency and divide by sum of freqencies of all words\n",
    "        \"\"\"\n",
    "        locations = self.classes + ['Total',]\n",
    "        word_locality_features = {label: defaultdict() for label in locations}\n",
    "        word_locality_weight = {label: defaultdict() for label in self.classes}\n",
    "        \n",
    "        for x_i, y_i in zip(X.index, y.index):\n",
    "            text = X.loc[x_i].split()\n",
    "            for word in text:\n",
    "                word_locality_features[y.loc[y_i]][word] += 1\n",
    "                word_locality_features[y.loc[y_i]]['Total'] += 1\n",
    "                word_locality_features['Total'][word] += 1\n",
    "                word_locality_features['Total']['Total'] += 1\n",
    "        \n",
    "        for label in self.classes:\n",
    "            for word in word_locality_features[label].keys():\n",
    "                cond_word_prob = word_locality_features[label][word] / word_locality_features[label]['Total']\n",
    "                word_prob = word_locality_features['Total'][word] / word_locality_features['Total']['Total']\n",
    "                word_locality_weight[label][word] = cond_word_prob / word_prob\n",
    "        \n",
    "        features = []\n",
    "        for location in self.classes:\n",
    "            n_location_features = int(self.n_features / len(self.classes))\n",
    "            features.append(sorted(word_locality_weight[location].items(), key=lambda kv: kv[1], reverse=True)[:n_location_features]) \n",
    "        \n",
    "        self.features = set([feature[0] for feature in features])\n",
    "                \n",
    "    \n",
    "    def information_gain_ratio(self, x):\n",
    "#         return info_gain_ratio\n",
    "        pass\n",
    "    \n",
    "    def term_frequency_inverse_city_frequency(self, X, y):\n",
    "        vectorizer = TfidfVectorizer(stop_words=self.stop_words, max_features=self.n_features)\n",
    "\n",
    "        location_word_list = {label: '' for label in self.classes}\n",
    "\n",
    "        for x_i, y_i in zip(X.index, y.index):\n",
    "            location_word_list[y.loc[y_i]] += X.loc[x_i] + \" \"\n",
    "        \n",
    "        labels = location_word_list.keys()\n",
    "        corpus = location_word_list.values()\n",
    "        vectorizer.fit(corpus, labels)\n",
    "        self.features = set(vectorizer.get_feature_names())\n",
    "        \n",
    "    def filter(self, x):\n",
    "        return x.apply(self.filter_text)\n",
    "    \n",
    "    def filter_text(self, text):\n",
    "        return ' '.join(self.filter_word(w) for w in text.split())\n",
    "        \n",
    "    def filter_word(self, word):\n",
    "        word = word.lower()\n",
    "        # extract keywords from hashtag \n",
    "\n",
    "        if self._is_hyperlink(word):\n",
    "            return ''\n",
    "        elif self._is_hashtag(word):\n",
    "            word = self._process_hashtag(word)\n",
    "        # potentially cross-reference individuals mentioned? or discard\n",
    "        elif self._is_mention(word):\n",
    "            word = self._process_mention(word)\n",
    "        # remove ascii characters \n",
    "        else:\n",
    "            word = self._ascii_to_unicode(word)\n",
    "            \n",
    "            word = re.sub(r'[^\\w\\s]',' ', word)\n",
    "#             word = self._word_stem(word)\n",
    "            if word in self.stop_words:\n",
    "                return ''\n",
    "        return word\n",
    "                \n",
    "    def _is_hashtag(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"#\"\n",
    "    \n",
    "    def _is_mention(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"@\"\n",
    "    \n",
    "    def _is_hyperlink(self, word):\n",
    "        return validators.url(word)\n",
    "    \n",
    "    def _process_hashtag(self, word):\n",
    "        return word[1:]\n",
    "    \n",
    "    def _process_mention(self, word):\n",
    "        return word[1:]\n",
    "    \n",
    "    def _ascii_to_unicode(self, word):\n",
    "        for uescape in re.findall(r'(\\\\u[0-9a-f]{4})', word):\n",
    "            try:\n",
    "                word = word.replace(uescape, '')  \n",
    "            except (UnicodeDecodeError, Exception):\n",
    "                print(\"Failed to decode: {}\".format(uescape))\n",
    "        return word\n",
    "    \n",
    "    def _word_stem(self, word):\n",
    "        return self.stemmer.stem(word)\n",
    "\n",
    "class MetaClassifier:\n",
    "    def __init__(self, estimators, random_state):\n",
    "        self.estimators = estimators\n",
    "        self.encoder = OneHotEncoder()\n",
    "        self.base_classifier = svm.LinearSVC(random_state=random_state)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        y_set = pd.DataFrame(index=X.index, columns=[item[0] for item in self.estimators])\n",
    "        \n",
    "        for name, classifier in self.estimators:\n",
    "            classifier.fit(X, y)\n",
    "            y_bar = classifier.predict(X)\n",
    "            y_set[name] = self.encoder.fit_transform(y_bar.reshape(-1, 1)).toarray()\n",
    "            \n",
    "        self.base_classifier.fit(y_set, y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_set = pd.DataFrame(index=X.index, columns=[item[0] for item in self.estimators])\n",
    "\n",
    "        for name, classifier in self.estimators:\n",
    "            y_bar = classifier.predict(X)\n",
    "            y_set[name] = self.encoder.transform(y_bar.reshape(-1, 1)).toarray()\n",
    "                \n",
    "        return  self.base_classifier.predict(y_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['One-R', 'Decision-Tree', 'MultinomialNB', 'LinearSVM']\n",
      "Time taken: 0:08:24.903216\n",
      "Time taken: 0:02:37.557461\n",
      "Time taken: 0:03:34.017542\n",
      "Time taken: 0:04:44.567360\n"
     ]
    }
   ],
   "source": [
    "# classifier_set = [\"One-R\"]\n",
    "classifier_set = [\"LinearSVM\", \"MultinomialNB\"]\n",
    "print(classifier_set)\n",
    "voting_strategy = \"simple_voting\"\n",
    "combine_classifiers = False\n",
    "\n",
    "gt = GeoTagger(\n",
    "    inputs = inputs,\n",
    "    target = output,\n",
    "    classifier_set = classifier_set,\n",
    "    ensemble_strategy = voting_strategy,\n",
    "    feature_selection_method = \"tf_idf\",\n",
    "    seed = 500,\n",
    "    combine_classifiers = combine_classifiers\n",
    ")\n",
    "\n",
    "start = datetime.now()\n",
    "gt.train(x_train, y_train)\n",
    "end = datetime.now()\n",
    "print(\"Time taken: {}\".format(end - start))\n",
    "\n",
    "gt2 = GeoTagger(\n",
    "    inputs = inputs,\n",
    "    target = output,\n",
    "    classifier_set = classifier_set,\n",
    "    ensemble_strategy = voting_strategy,\n",
    "    feature_selection_method = \"baseline_10\",\n",
    "    seed = 500,\n",
    "    combine_classifiers = combine_classifiers\n",
    ")\n",
    "\n",
    "start = datetime.now()\n",
    "gt2.train(x_train, y_train)\n",
    "end = datetime.now()\n",
    "print(\"Time taken: {}\".format(end - start))\n",
    "\n",
    "gt3 = GeoTagger(\n",
    "    inputs = inputs,\n",
    "    target = output,\n",
    "    classifier_set = classifier_set,\n",
    "    ensemble_strategy = voting_strategy,\n",
    "    feature_selection_method = \"baseline_50\",\n",
    "    seed = 500,\n",
    "    combine_classifiers = combine_classifiers\n",
    ")\n",
    "\n",
    "start = datetime.now()\n",
    "gt3.train(x_train, y_train)\n",
    "end = datetime.now()\n",
    "print(\"Time taken: {}\".format(end - start))\n",
    "\n",
    "gt4 = GeoTagger(\n",
    "    inputs = inputs,\n",
    "    target = output,\n",
    "    classifier_set = classifier_set,\n",
    "    ensemble_strategy = voting_strategy,\n",
    "    feature_selection_method = \"baseline_100\",\n",
    "    seed = 500,\n",
    "    combine_classifiers = combine_classifiers\n",
    ")\n",
    "\n",
    "start = datetime.now()\n",
    "gt4.train(x_train, y_train)\n",
    "end = datetime.now()\n",
    "print(\"Time taken: {}\".format(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the class labels of a set of test data\n",
    "ybars = gt.predict(x_dev)\n",
    "ybars2 = gt2.predict(x_dev)\n",
    "ybars3 = gt3.predict(x_dev)\n",
    "ybars4 = gt4.predict(x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [gt, gt2, gt3, gt4]\n",
    "# predictions = [ybars, ybars2, ybars3, ybars4]\n",
    "# evals = [\"accuracy\", \"precision_recall_f-score_with_micro\"]\n",
    "\n",
    "# for model, prediction in zip(models, predictions):\n",
    "#     for method in evals:\n",
    "#         report, confusion = model.evaluate(prediction, y_dev, method)\n",
    "#         print(\"{}: {}\".format(method, report, ))\n",
    "accScores = gt.evaluate(ybars, y_dev)\n",
    "# otherScores = gt.evaluate(ybars, y_dev, \"precision_recall_f-score_with_micro\")\n",
    "accScores2 = gt2.evaluate(ybars2, y_dev)\n",
    "# otherScores2 = gt2.evaluate(ybars2, y_dev, \"precision_recall_f-score_with_micro\")\n",
    "accScores3 = gt3.evaluate(ybars3, y_dev)\n",
    "# otherScores3 = gt3.evaluate(ybars3, y_dev, \"precision_recall_f-score_with_micro\")\n",
    "accScores4 = gt4.evaluate(ybars4, y_dev)\n",
    "# otherScores4 = gt4.evaluate(ybars4, y_dev, \"precision_recall_f-score_with_micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'One-R': 0.2693029490616622, 'Decision-Tree': 0.3017426273458445, 'MultinomialNB': 0.31847184986595173, 'LinearSVM': 0.3174530831099196}) defaultdict(None, {'One-R': 0.2693029490616622, 'Decision-Tree': 0.3152278820375335, 'MultinomialNB': 0.3106166219839142, 'LinearSVM': 0.31549597855227884}) defaultdict(None, {'One-R': 0.2693029490616622, 'Decision-Tree': 0.31056300268096515, 'MultinomialNB': 0.3155495978552279, 'LinearSVM': 0.3202412868632708}) defaultdict(None, {'One-R': 0.2693029490616622, 'Decision-Tree': 0.3124128686327078, 'MultinomialNB': 0.326970509383378, 'LinearSVM': 0.3225469168900804})\n"
     ]
    }
   ],
   "source": [
    "print(accScores, accScores2, accScores3, accScores4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array(['Brisbane', 'Melbourne', 'Perth', 'Sydney'], dtype=object), array([62648, 27532, 26591, 32429], dtype=int64))\n",
      "(array(['Brisbane', 'Melbourne', 'Perth', 'Sydney'], dtype=object), array([71147, 57048, 15850,  5155], dtype=int64))\n",
      "(array(['Brisbane', 'Melbourne', 'Perth', 'Sydney'], dtype=object), array([64150, 22852, 11545, 50653], dtype=int64))\n",
      "(array(['Brisbane', 'Melbourne', 'Perth', 'Sydney'], dtype=object), array([69120, 18512, 19236, 42332], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(np.unique(ybars, return_counts=True))\n",
    "print(np.unique(ybars2, return_counts=True))\n",
    "print(np.unique(ybars3, return_counts=True))\n",
    "print(np.unique(ybars4, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>One-R</th>\n",
       "      <th>Decision-Tree</th>\n",
       "      <th>MultinomialNB</th>\n",
       "      <th>LinearSVM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Perth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Perth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Melbourne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Melbourne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Brisbane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Brisbane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Perth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Brisbane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Perth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Melbourne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Melbourne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Perth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37270</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Perth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37271</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37272</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Melbourne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37273</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Melbourne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37274</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37275</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37276</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37277</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37278</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37279</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37280</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37281</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Melbourne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37282</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Perth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37283</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Melbourne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37284</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Melbourne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37285</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37286</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37287</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Perth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37288</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37289</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Brisbane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37290</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37291</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Perth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37292</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37293</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Melbourne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37294</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Sydney</td>\n",
       "      <td>Perth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37295</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37296</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Brisbane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37297</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Melbourne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37298</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Melbourne</td>\n",
       "      <td>Melbourne</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37299</th>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Perth</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Sydney</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37300 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          One-R Decision-Tree MultinomialNB  LinearSVM\n",
       "0      Brisbane         Perth      Brisbane     Sydney\n",
       "1      Brisbane        Sydney         Perth     Sydney\n",
       "2      Brisbane      Brisbane      Brisbane   Brisbane\n",
       "3      Brisbane         Perth        Sydney      Perth\n",
       "4      Brisbane      Brisbane      Brisbane   Brisbane\n",
       "5      Brisbane      Brisbane     Melbourne      Perth\n",
       "6      Brisbane     Melbourne     Melbourne  Melbourne\n",
       "7      Brisbane         Perth      Brisbane     Sydney\n",
       "8      Brisbane         Perth      Brisbane   Brisbane\n",
       "9      Brisbane      Brisbane        Sydney  Melbourne\n",
       "10     Brisbane     Melbourne         Perth   Brisbane\n",
       "11     Brisbane     Melbourne        Sydney   Brisbane\n",
       "12     Brisbane     Melbourne        Sydney     Sydney\n",
       "13     Brisbane        Sydney      Brisbane   Brisbane\n",
       "14     Brisbane         Perth     Melbourne      Perth\n",
       "15     Brisbane      Brisbane      Brisbane   Brisbane\n",
       "16     Brisbane        Sydney        Sydney     Sydney\n",
       "17     Brisbane         Perth        Sydney     Sydney\n",
       "18     Brisbane      Brisbane        Sydney   Brisbane\n",
       "19     Brisbane      Brisbane         Perth      Perth\n",
       "20     Brisbane      Brisbane      Brisbane   Brisbane\n",
       "21     Brisbane        Sydney     Melbourne  Melbourne\n",
       "22     Brisbane     Melbourne        Sydney     Sydney\n",
       "23     Brisbane        Sydney     Melbourne  Melbourne\n",
       "24     Brisbane      Brisbane         Perth     Sydney\n",
       "25     Brisbane      Brisbane        Sydney     Sydney\n",
       "26     Brisbane     Melbourne        Sydney      Perth\n",
       "27     Brisbane        Sydney      Brisbane   Brisbane\n",
       "28     Brisbane         Perth      Brisbane   Brisbane\n",
       "29     Brisbane      Brisbane        Sydney     Sydney\n",
       "...         ...           ...           ...        ...\n",
       "37270  Brisbane     Melbourne         Perth      Perth\n",
       "37271  Brisbane     Melbourne        Sydney     Sydney\n",
       "37272  Brisbane     Melbourne     Melbourne  Melbourne\n",
       "37273  Brisbane        Sydney     Melbourne  Melbourne\n",
       "37274  Brisbane         Perth     Melbourne     Sydney\n",
       "37275  Brisbane         Perth      Brisbane     Sydney\n",
       "37276  Brisbane     Melbourne        Sydney     Sydney\n",
       "37277  Brisbane        Sydney         Perth     Sydney\n",
       "37278  Brisbane         Perth      Brisbane     Sydney\n",
       "37279  Brisbane      Brisbane      Brisbane      Perth\n",
       "37280  Brisbane      Brisbane         Perth     Sydney\n",
       "37281  Brisbane         Perth     Melbourne  Melbourne\n",
       "37282  Brisbane      Brisbane        Sydney      Perth\n",
       "37283  Brisbane        Sydney     Melbourne  Melbourne\n",
       "37284  Brisbane         Perth     Melbourne  Melbourne\n",
       "37285  Brisbane         Perth        Sydney     Sydney\n",
       "37286  Brisbane         Perth        Sydney     Sydney\n",
       "37287  Brisbane         Perth         Perth      Perth\n",
       "37288  Brisbane      Brisbane         Perth     Sydney\n",
       "37289  Brisbane      Brisbane         Perth   Brisbane\n",
       "37290  Brisbane     Melbourne        Sydney     Sydney\n",
       "37291  Brisbane     Melbourne         Perth      Perth\n",
       "37292  Brisbane      Brisbane        Sydney     Sydney\n",
       "37293  Brisbane      Brisbane        Sydney  Melbourne\n",
       "37294  Brisbane     Melbourne        Sydney      Perth\n",
       "37295  Brisbane      Brisbane      Brisbane   Brisbane\n",
       "37296  Brisbane      Brisbane      Brisbane   Brisbane\n",
       "37297  Brisbane     Melbourne      Brisbane  Melbourne\n",
       "37298  Brisbane         Perth     Melbourne  Melbourne\n",
       "37299  Brisbane         Perth      Brisbane     Sydney\n",
       "\n",
       "[37300 rows x 4 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Resources\n",
    "- https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
