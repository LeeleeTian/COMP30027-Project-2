{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Build Baseline Location Classifier\\nApproaches:\\n- instance representation in form of 'bag' of words \\n    - features: word frequencies, metadata \\n    - exclude rare words in data set (used by less than 3 users )\\n- gramatical structure with NLP\\n- model instances in terms of authors instead of documents \\n\\n\\n- Baseline Classifier: Naive Bayes Model ()\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Build Baseline Location Classifier\n",
    "Approaches:\n",
    "- instance representation in form of 'bag' of words \n",
    "    - features: word frequencies, metadata \n",
    "    - exclude rare words in data set (used by less than 3 users )\n",
    "- gramatical structure with NLP\n",
    "- model instances in terms of authors instead of documents \n",
    "\n",
    "\n",
    "- Baseline Classifier: Naive Bayes Model ()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from os import path, getcwd\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re, string\n",
    "import validators\n",
    "from info_gain.info_gain import info_gain_ratio\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# nltk.download('english')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Instance_ID      Location        Text\n",
      "31      ?  Awesome @evomagazine cover, time to dive in ye...\n",
      "32      ?                        @InceptCruze goes 4-30 smfh\n",
      "33      ?  Sad loss to business &amp; sailing worlds. #RI...\n",
      "34      ?  @heavenlogin I would like my pokemon back in p...\n",
      "35      ?                        #Hit30 Girls Talk Boys 5SOS\n",
      "36      ?                 *X-ellent  https://t.co/oDtqUD8H1i\n",
      "37      ?  Dean Braun. Take a bow. How good are your runn...\n",
      "38      ?  ***sigh***  Not sure which coverage is going t...\n",
      "39      ?  public dm'yi kapattim kapatali cirkin kizlarda...\n",
      "310     ?  @fullcammunism it's an American smoker/chargri...\n",
      "311     ?              @Kon__K You're being a bit cruel now.\n",
      "312     ?  My buddy, the ibis. ANZAC Square, Brisbane. @ ...\n",
      "313     ?  Other. Blacktown (M7 (Westlink Mtwy), Quakers ...\n",
      "314     ?  @lukeamac @MattJDavies74 Same clown came up wi...\n",
      "315     ?         @itsHIMYMquotes why don't I remember this?\n",
      "316     ?  .@StephenKing, it must be very tempting for @N...\n",
      "317     ?            Well Said Ross. https://t.co/07cFZSFLmR\n",
      "318     ?  If Wells had a durable body we'd be talking ab...\n",
      "319     ?  Foolproof https://t.co/cIZwxLDCSY #weber #duck...\n",
      "320     ?  A win here could see the Fire, ninth, move wit...\n",
      "321     ?  #NEW. Large apartment in one of the most desir...\n",
      "322     ?  @qielachvn__ @fatinanuar @jiawhoo SEK MU TAHU ...\n",
      "323     ?  @rohan_connolly tv ratings?  Brick manufacture...\n",
      "324     ?                     @NarreWarrenFNC LOL. OK Narre.\n",
      "325     ?  #ManhattanTerrace #24/7 #SafeSex https://t.co/...\n",
      "326     ?                  Well damn https://t.co/9cutZ0oEMJ\n",
      "327     ?     Send it!  @jon30591293 https://t.co/J8AuZfWSdd\n",
      "328     ?  @RKJ_AK BOI shud accept Hrithik and SRK are on...\n",
      "329     ?  @junkeedotcom Your'e incredibly ignorant,racis...\n",
      "330     ?  @ragon33 @CombatFuzzy Are there any mess or so...\n",
      "...                                                      ...\n",
      "3108119 ?  Easy to see where the support is tonight... #A...\n",
      "3108120 ?  Kaptid mars tlga whahahaha #ALDUBLolanap https...\n",
      "3108121 ?  #auspolvotes is now trending in #Melbourne htt...\n",
      "3108122 ?  Good morning #Brisbane! Sun will rise in 30 mi...\n",
      "3108123 ?  It was a huge pleasure having time tonight wit...\n",
      "3108124 ?  All  aged care workers want for Christmas is a...\n",
      "3108125 ?                                          goodnight\n",
      "3108126 ?  @chelsea_hunter @GuiltFemPod am going to #Podl...\n",
      "3108127 ?  #Kabali #Cantwait #Sydney @dhanushkraja @sound...\n",
      "3108128 ?             @jraeanna Yup. It's all over the news.\n",
      "3108129 ?                             @IainnB \\innovation\\\"\"\n",
      "3108130 ?             good sleep last night with moz and tay\n",
      "3108131 ?   7 weeks till I move to Sydney... Fuck how scary!\n",
      "3108132 ?                 There's some superb actors in this\n",
      "3108133 ?  New trade Sell US Dow Jones  Index CFDs  @ 191...\n",
      "3108134 ?  Waiting patiently for my flight home @ Brisban...\n",
      "3108135 ?                            https://t.co/tgeJJp6oYh\n",
      "3108136 ?                    FFS...  https://t.co/PurfX3ujG6\n",
      "3108137 ?  @MelhamP3 thanks for the photo, you were brill...\n",
      "3108138 ?  @TwitWhatter you shouldnt talk about yourself ...\n",
      "3108139 ?         Malas nak bercakap tapi faham sendiri jela\n",
      "3108140 ?  @vmainard @Loud_Lass Sorry but wrong again. Th...\n",
      "3108141 ?  I'm at Oko Oko in Carlton, Vic https://t.co/97...\n",
      "3108142 ?  Thx for following @reecejhawaii, never been to...\n",
      "3108143 ?  @godnojoe @OnceUponADavid correct. Not having ...\n",
      "3108144 ?  We shall soon see what matters more ;) #nocomment\n",
      "3108145 ?  @brettk351 @johndory49   Run it again, twice, ...\n",
      "3108146 ?  @NforNihilism everyone dies and Darth Vader is...\n",
      "3108147 ?  Choose the reef, not coal. Yep. #auspol #elect...\n",
      "3108148 ?  This evenings adventure...wombat back at the d...\n",
      "\n",
      "[108142 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "fdir = path.join(getcwd(), \"2019S1-proj2-datah\")\n",
    "\n",
    "train_data = \"train-raw.tsv\"\n",
    "test_data = \"test-raw.tsv\"\n",
    "dev_data = \"dev-raw.tsv\"\n",
    "\n",
    "train_fpath = path.join(fdir, train_data)\n",
    "test_fpath = path.join(fdir, test_data)\n",
    "dev_fpath = path.join(fdir, dev_data)\n",
    "\n",
    "train = pd.read_csv(train_fpath, encoding=\"utf_8\", delimiter=\"\\t\", index_col=\"Instance_ID\")\n",
    "test = pd.read_csv(test_fpath, encoding=\"utf_8\", delimiter=\"\\t\") #, index_col=\"Instance_ID\")\n",
    "dev = pd.read_csv(dev_fpath, encoding=\"utf_8\", delimiter=\"\\t\")#, index_col=\"Instance_ID\")\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Text'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-261-c71162dc464e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mx_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Text'"
     ]
    }
   ],
   "source": [
    "inputs = 'Text'\n",
    "output = 'Location'\n",
    "x_train = train[inputs]\n",
    "y_train = train[output]\n",
    "x_test = test[inputs]\n",
    "y_test = test[output]\n",
    "x_dev = dev[inputs]\n",
    "y_dev = dev[output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTagger:\n",
    "    _FEATURE_SELECTION = [\"baseline_10\", \"baseline_50\", \"baseline_100\", \"info_gain_ratio\", \"word_locality_heuristic\", \"tf_idf\"]\n",
    "    _VOTING_STRATEGY = [\"simple_voting\", \"bagging\", \"stacking\", \"random_forest\", \"boosting\"]\n",
    "    _CLASSIFIERS = [\"Zero-R\", \"One-R\", \"Decision-Tree\", \"MultinomialNB\", \"LinearSVM\", \"SemiSupervised\"]\n",
    "    _EVALUATION_METRIC = [\"accuracy\", \"precision_recall_f-score_with_macro\", \"precision_recall_f-score_with_micro\"]\n",
    "    \n",
    "    def __init__(self, inputs, target, classifier_set=[\"MultinomialNB\"], voting_strategy=\"simple_voting\", feature_selection_method=\"baseline_100\", seed=500):\n",
    "        self.inputs = inputs\n",
    "        self.target = target\n",
    "        self.classifier_set = classifier_set\n",
    "        self.voting_strategy = voting_strategy\n",
    "        self.feature_selection_method = feature_selection_method\n",
    "        self.classifier_set = self._combine_classifier_set(classifier_set)\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        trains a classifier given the training data and their corresponding class labels\n",
    "        \"\"\"\n",
    "        self.classes = y.unique()\n",
    "        X = self.preprocess(X, y, train=True)\n",
    "        \n",
    "        for classifier in self.classifier_set.values():\n",
    "            print(type(classifier))\n",
    "            classifier.fit(X, y)\n",
    "          \n",
    "    def predict(self, X):\n",
    "        X = self.preprocess(X)\n",
    "        predictions = pd.DataFrame()\n",
    "                \n",
    "        for name, classifier in self.classifier_set.items():\n",
    "            classifier_prediction = classifier.predict(X, y)\n",
    "            predictions[name] = classifier_predictions\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, ybar, y, metric):\n",
    "        #TODO: eval method\n",
    "        if not metric in GeoTagger._EVALUATION_METRIC:\n",
    "                print(\"Invalid Evaluation Metric: {}. Choose one of \\\n",
    "                ({})\".format(metric, \", \".join(GeoTagger._EVALUATION_METRIC))) \n",
    "                return\n",
    "            \n",
    "        if metric == \"accuracy\":\n",
    "            score = accuracy_score(ybar, y)\n",
    "        if metric == \"precision\":\n",
    "            score = precision_recall_fscore_support(y_true, y_pred, average='macro'),\n",
    "        if metric == \"precision_recall_f-score_with_micro\":\n",
    "            score = precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "            \n",
    "        return score\n",
    "    \n",
    "    def preprocess(self, X, y=None, train=False):\n",
    "        \"\"\"\n",
    "         - Filter rare words (urls, typos rare names, punctuation symbols)\n",
    "         - calculate word frequencies \n",
    "         - metadata\n",
    "        \"\"\"\n",
    "        X = self.filter(X)\n",
    "\n",
    "        if train:\n",
    "            self.feature_selection(X, y)\n",
    "        \n",
    "        X = self.bag_of_words(X)\n",
    "            \n",
    "        return X \n",
    "    \n",
    "    def bag_of_words(self, X):\n",
    "        _x = pd.DataFrame(\n",
    "            [[(word in text) for word in sorted(list(self.features))] for text in X.values],\n",
    "            index=X.index, \n",
    "            columns=self.features,\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        print(_x)\n",
    "        \n",
    "        return _x\n",
    "    \n",
    "    def _combine_classifier_set(self, classifiers):\n",
    "        classifier_set = defaultdict()\n",
    "        \n",
    "        for classifier in classifiers:\n",
    "            if not classifier in GeoTagger._CLASSIFIERS:\n",
    "                print(\"Invalid Classifier: {}. Choose one of \\\n",
    "                ({})\".format(classifier, \", \".join(GeoTagger._CLASSIFIERS)))\n",
    "                continue\n",
    "                \n",
    "            if classifier == \"Zero-R\":\n",
    "                classifier_set[classifier] = DummyClassifier(strategy='most_frequent', random_state=self.seed)\n",
    "            elif classifier == \"One-R\":\n",
    "                classifier_set[classifier] = DecisionTreeClassifier(max_depth=1, criterion=\"entropy\", random_state=self.seed)\n",
    "            elif classifier == \"Decision-Tree\":\n",
    "                classifier_set[classifier] = DecisionTreeClassifier(max_depth=None, criterion=\"entropy\", random_state=self.seed)\n",
    "            elif classifier == \"MultinomialNB\":\n",
    "                classifier_set[classifier] = MultinomialNB()\n",
    "            elif classifier == \"LinearSVM\":\n",
    "                continue\n",
    "            elif classifier == \"SemiSupervised\":\n",
    "                continue\n",
    "\n",
    "        return classifier_set\n",
    "                \n",
    "    def feature_selection(self, X, y):\n",
    "        \"\"\"\n",
    "        (1) Information Gain Ratio (IGR) - across all states S, is \n",
    "            defined as the ratio between its information gain value IG, \n",
    "            which measures the decrease in class entropy H that w brings,\n",
    "            and its intrinsic entropy IV, which measures the entropy of \n",
    "            the presence versus the absence of that word\n",
    "            \n",
    "        (2) Word Locality Heuristic (WLH) - promotes words primarily \n",
    "            associated with one location. measure the probability of \n",
    "            a word occurring in a state, divided by its probability to \n",
    "            appear in any state. Then, for a given word w, we define the \n",
    "            WLH as the maximum such probability across all the states S\n",
    "        \"\"\"\n",
    "        if self.feature_selection_method not in GeoTagger._FEATURE_SELECTION:\n",
    "            print(\"Invalid Feature Selection method: {}. Choose one of \\\n",
    "            ({})\".format(self.feature_selection_method, \", \".join(GeoTagger._FEATURE_SELECTION)))\n",
    "            return \n",
    "        \n",
    "        if self.feature_selection_method == \"baseline_10\":\n",
    "            self.baseline_heuristic(X, y, \"10\")\n",
    "        elif self.feature_selection_method == \"baseline_50\":\n",
    "            self.baseline_heuristic(X, y, \"50\")\n",
    "        elif self.feature_selection_method == \"baseline_100\":\n",
    "            self.baseline_heuristic(X, y, \"100\")\n",
    "        elif self.feature_selection_method == \"info_gain_ratio\":\n",
    "#             self.information_gain_ratio(x)\n",
    "            return\n",
    "        elif self.feature_selection_method == \"word_locality_heuristic\":\n",
    "            self.word_locality_weight(X, y)\n",
    "        elif self.feature_selection_method == \"tf_idf\":\n",
    "            self.tf_idf(X, y)\n",
    "\n",
    "    def baseline_heuristic(self, X, y, top_n):\n",
    "#         fdir = path.join(getcwd(), \"2019S1-proj2-datah\")\n",
    "        feature_fpath = path.join(fdir, \"train-top\" + top_n + \".csv\")\n",
    "        \n",
    "        if not path.exists(feature_fpath):\n",
    "            print(\"Baseline Heuristic path {} does not exist\".format(feature_fpath))\n",
    "            return\n",
    "        \n",
    "        features = open(feature_fpath).readline()\n",
    "        features = features.split(\",\")\n",
    "        features.remove(\"Instance_ID\")\n",
    "        features.remove(\"Location\\n\")\n",
    "        self.features = set(features)\n",
    "\n",
    "    def word_locality_weight(self, X, y):\n",
    "        \"\"\"\n",
    "        calculate frequencies of data \n",
    "        Measure frequency and divide by sum of freqencies of all words\n",
    "        \"\"\"\n",
    "        locations = self.classes + ['Total',]\n",
    "        word_location_weight = {label: defaultdict() for label in locations}\n",
    "        \n",
    "        for x_i, y_i in zip(X.index, y.index):\n",
    "            text = X.loc[x_i].split()\n",
    "            for word in text:\n",
    "                word_location_weight[y.loc[y_i]][word] += 1\n",
    "                word_location_weight[y.loc[y_i]]['Total'] += 1\n",
    "                word_location_weight['Total'][word] += 1\n",
    "                word_location_weight['Total']['Total'] += 1\n",
    "                \n",
    "    \n",
    "    def information_gain_ratio(self, x):\n",
    "#         return info_gain_ratio\n",
    "        pass\n",
    "    \n",
    "    def tf_idf(self, X, y):\n",
    "        vectorizer = TfidfVectorizer(stop_words=self.stop_words, max_features=400)\n",
    "\n",
    "        location_word_list = {label: '' for label in self.classes}\n",
    "\n",
    "        for x_i, y_i in zip(X.index, y.index):\n",
    "            location_word_list[y.loc[y_i]] += X.loc[x_i] + \" \"\n",
    "        \n",
    "        labels = location_word_list.keys()\n",
    "        corpus = location_word_list.values()\n",
    "        vectorizer.fit(corpus, labels)\n",
    "        self.features = set(vectorizer.get_feature_names())\n",
    "        \n",
    "    def filter(self, x):\n",
    "        return x.apply(self.filter_text)\n",
    "    \n",
    "    def filter_text(self, text):\n",
    "        return ' '.join(self.filter_word(w) for w in text.split())\n",
    "        \n",
    "    def filter_word(self, word):\n",
    "        word = word.lower()\n",
    "        # extract keywords from hashtag \n",
    "        if self._is_hyperlink(word):\n",
    "            return ''\n",
    "        elif self._is_hashtag(word):\n",
    "            word = self._process_hashtag(word)\n",
    "        # potentially cross-reference individuals mentioned? or discard\n",
    "        elif self._is_mention(word):\n",
    "            word = self._process_mention(word)\n",
    "        # remove ascii characters \n",
    "        else:\n",
    "            word = self._ascii_to_unicode(word)\n",
    "            word = self._word_stem(word)\n",
    "            word = re.sub(r'[^\\w\\s]',' ', word)\n",
    "        return word\n",
    "                \n",
    "    def _is_hashtag(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"#\"\n",
    "    \n",
    "    def _is_mention(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"@\"\n",
    "    \n",
    "    def _is_hyperlink(self, word):\n",
    "        return validators.url(word)\n",
    "    \n",
    "    def _process_hashtag(self, word):\n",
    "        return word[1:]\n",
    "    \n",
    "    def _process_mention(self, word):\n",
    "        return word[1:]\n",
    "    \n",
    "    def _ascii_to_unicode(self, word):\n",
    "        for uescape in re.findall(r'(\\\\u[0-9a-f]{4})', word):\n",
    "            try:\n",
    "#                 print(uescape.encode('utf-8').decode('unicode-escape'), type(uescape.encode('utf-8').decode('unicode-escape')))\n",
    "#                 word = re.sub(uescape, uescape.encode('utf-8'), word)\n",
    "#                 print(word)\n",
    "#                 print(uescape, type(uescape))\n",
    "                word = word.replace(uescape, '')  \n",
    "            except (UnicodeDecodeError, Exception):\n",
    "                print(\"Failed to decode: {}\".format(uescape))\n",
    "        return word\n",
    "    \n",
    "    def _word_stem(self, word):\n",
    "        return self.stemmer.stem(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        put  would  co  hate  sad  yes  melbourn  wish  give  definit  ...  \\\n",
      "0         0      0   0     0    0    0         0     0     0        0  ...   \n",
      "1         0      0   0     0    0    0         0     0     0        0  ...   \n",
      "2         0      0   0     0    0    0         0     0     0        0  ...   \n",
      "3         0      0   0     0    0    0         0     0     0        0  ...   \n",
      "4         0      1   0     0    0    0         0     0     0        0  ...   \n",
      "5         0      0   0     0    0    0         0     0     0        0  ...   \n",
      "6         0      0   0     0    0    0         0     0     0        0  ...   \n",
      "7         0      0   0     0    0    0         0     0     0        0  ...   \n",
      "8         0      0   0     0    0    0         0     0     0        0  ...   \n",
      "9         0      0   0     0    0    0         0     0     0        0  ...   \n",
      "10        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "11        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "12        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "13        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "14        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "15        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "16        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "17        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "18        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "19        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "20        1      0   0     1    0    0         0     0     0        0  ...   \n",
      "21        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "22        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "23        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "24        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "25        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "26        0      0   0     1    0    0         0     0     0        0  ...   \n",
      "27        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "28        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "29        0      0   0     0    0    0         0     0     0        0  ...   \n",
      "...     ...    ...  ..   ...  ...  ...       ...   ...   ...      ...  ...   \n",
      "103330    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103331    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103332    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103333    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103334    0      0   0     0    0    1         0     0     0        1  ...   \n",
      "103335    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103336    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103337    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103338    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103339    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103340    0      0   0     0    0    0         0     0     1        0  ...   \n",
      "103341    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103342    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103343    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103344    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103345    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103346    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103347    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103348    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103349    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103350    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103351    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103352    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103353    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103354    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103355    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103356    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103357    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103358    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "103359    0      0   0     0    0    0         0     0     0        0  ...   \n",
      "\n",
      "        rain  hi  save  dog  read  brisbane  got  nsw  congratul  one  \n",
      "0          0   0     0    0     0         0    0    0          0    0  \n",
      "1          0   0     0    0     0         0    0    0          0    0  \n",
      "2          0   0     0    0     0         0    0    0          0    0  \n",
      "3          0   0     0    0     0         0    0    0          0    0  \n",
      "4          0   0     0    0     0         0    0    0          0    0  \n",
      "5          0   0     0    0     0         0    0    0          0    0  \n",
      "6          0   0     0    0     0         0    0    0          0    0  \n",
      "7          0   1     0    0     0         0    0    0          0    0  \n",
      "8          0   0     0    0     0         0    0    0          0    0  \n",
      "9          0   0     0    0     0         0    0    0          0    0  \n",
      "10         0   0     0    0     0         0    0    0          0    0  \n",
      "11         0   0     0    0     0         0    0    0          0    0  \n",
      "12         0   0     0    0     0         0    0    0          0    0  \n",
      "13         0   0     0    0     0         0    0    0          0    0  \n",
      "14         0   0     0    0     0         0    0    0          0    0  \n",
      "15         0   0     0    0     0         0    0    0          0    0  \n",
      "16         0   0     0    0     0         0    0    0          0    0  \n",
      "17         0   0     0    0     0         0    0    0          0    0  \n",
      "18         0   0     0    0     0         0    0    0          0    0  \n",
      "19         0   0     0    0     0         0    0    0          0    0  \n",
      "20         0   0     0    0     0         0    0    0          0    0  \n",
      "21         0   0     0    0     0         0    0    0          0    0  \n",
      "22         0   0     0    0     0         0    0    0          0    0  \n",
      "23         0   0     0    0     0         0    0    0          0    0  \n",
      "24         0   0     0    0     0         0    0    0          0    0  \n",
      "25         0   0     0    0     0         0    0    0          0    0  \n",
      "26         0   0     0    0     0         0    0    0          0    0  \n",
      "27         0   0     0    0     0         0    0    0          0    0  \n",
      "28         0   0     0    0     0         0    0    0          0    0  \n",
      "29         0   0     0    0     0         0    0    0          0    0  \n",
      "...      ...  ..   ...  ...   ...       ...  ...  ...        ...  ...  \n",
      "103330     0   0     0    0     0         0    0    0          0    0  \n",
      "103331     0   0     0    0     0         0    0    0          0    0  \n",
      "103332     0   0     0    0     0         0    0    0          0    0  \n",
      "103333     0   0     0    0     0         0    0    0          0    0  \n",
      "103334     0   0     0    0     0         0    1    0          0    0  \n",
      "103335     0   0     0    0     0         0    0    0          0    0  \n",
      "103336     0   0     0    0     0         0    0    0          0    0  \n",
      "103337     0   0     0    0     0         0    1    1          0    0  \n",
      "103338     0   0     0    0     0         0    0    0          0    0  \n",
      "103339     0   0     0    0     0         0    0    0          0    0  \n",
      "103340     0   0     0    0     0         0    0    0          0    0  \n",
      "103341     0   0     0    0     0         0    0    0          0    0  \n",
      "103342     0   0     0    0     0         0    0    0          0    0  \n",
      "103343     0   0     0    0     0         0    0    0          0    0  \n",
      "103344     0   0     0    0     0         0    0    0          0    0  \n",
      "103345     0   0     0    0     0         0    0    0          0    0  \n",
      "103346     0   0     0    0     0         0    0    0          0    0  \n",
      "103347     0   0     0    0     0         0    0    0          0    0  \n",
      "103348     0   0     0    0     0         0    0    0          0    0  \n",
      "103349     0   0     0    0     0         0    0    0          0    0  \n",
      "103350     0   0     0    0     0         0    0    0          0    0  \n",
      "103351     0   0     0    0     0         0    0    0          0    0  \n",
      "103352     0   0     0    0     0         0    0    0          0    0  \n",
      "103353     0   0     0    0     0         0    0    0          0    0  \n",
      "103354     0   0     0    0     0         0    0    0          0    0  \n",
      "103355     0   0     0    0     0         0    0    0          0    0  \n",
      "103356     0   0     0    0     0         0    0    0          0    0  \n",
      "103357     0   0     0    0     0         0    0    0          0    0  \n",
      "103358     0   0     0    0     0         0    0    0          0    0  \n",
      "103359     0   0     0    0     0         0    0    0          0    0  \n",
      "\n",
      "[103360 rows x 400 columns]\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n"
     ]
    }
   ],
   "source": [
    "gt = GeoTagger(\n",
    "    inputs = inputs,\n",
    "    target = output,\n",
    "    classifier_set = [\"MultinomialNB\"], \n",
    "    voting_strategy = \"simple_voting\",\n",
    "    feature_selection_method = \"tf_idf\"\n",
    ")\n",
    "\n",
    "gt.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-258-28988b35c726>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#predict the class labels of a set of test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mybar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x_test' is not defined"
     ]
    }
   ],
   "source": [
    "#predict the class labels of a set of test data\n",
    "ybar = gt.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate classifier performance\n",
    "# score = self.evaluate(ybar, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Resources\n",
    "- https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
