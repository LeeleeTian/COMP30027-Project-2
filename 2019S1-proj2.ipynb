{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Build Baseline Location Classifier\\nApproaches:\\n- instance representation in form of 'bag' of words \\n    - features: word frequencies, metadata \\n    - exclude rare words in data set (used by less than 3 users )\\n- gramatical structure with NLP\\n- model instances in terms of authors instead of documents \\n\\n\\n- Baseline Classifier: Naive Bayes Model ()\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Build Baseline Location Classifier\n",
    "Approaches:\n",
    "- instance representation in form of 'bag' of words \n",
    "    - features: word frequencies, metadata \n",
    "    - exclude rare words in data set (used by less than 3 users )\n",
    "- gramatical structure with NLP\n",
    "- model instances in terms of authors instead of documents \n",
    "\n",
    "\n",
    "- Baseline Classifier: Naive Bayes Model ()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time \n",
    "from os import path, getcwd\n",
    "from collections import defaultdict\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "103310    Thanks Gillian, it was a wonderful day \\ud83c\\...\n",
       "103311    @Loki2896 @NorteGary @PaulABarter P.s. maximum...\n",
       "103312    @aaliaaaliya @DennisCricket_ Danny boy I'm in ...\n",
       "103313    You must pay, tell you where poltergeist. lock...\n",
       "103314    Eu assisti esse v\\u00eddeo e mentalizei como t...\n",
       "103315                                            Take care\n",
       "103316    Another season done with. These kids (7-9yo) w...\n",
       "103317                              https://t.co/ji7XRGGLds\n",
       "103318    If you see GnarMain69 and LuxMain69 in ranked ...\n",
       "103319    Isn\\u2019t great that I\\u2019m able to watch t...\n",
       "103320    @flobbit1 @terra155 @OddemocracyA Right, sure,...\n",
       "103321              @LegendValer Buang ka kalami ra sa dula\n",
       "103322    @DanishaCarter4 @Skrata2 Oh I saw it! Keep up ...\n",
       "103323    @TS7Track3 When Starlight was a surprise song ...\n",
       "103324    @MickKime @TonyAbbottMHR He speaks for so many...\n",
       "103325    @AntWadebridge @plalor Oh fuck off, no-one has...\n",
       "103326    @BuckAngel @allanamato \\ud83d\\udc9c\\ud83d\\udc9...\n",
       "103327    @NswPeter @quaedvliegs @phbarratt None....\\ bu...\n",
       "103328    i havent heard that so it doesnt count says Va...\n",
       "103329    Millsy has confirmed he does his own stunts. I...\n",
       "103330    Wow, I\\u2019m not very good at getting the pic...\n",
       "103331    I gather the Govt wants to further clog up Pun...\n",
       "103332    Visualization is the key to success. Dream big...\n",
       "103333    Countries I\\u2019ve Been To:  Fiji \\ud83c\\udde...\n",
       "103334    @TaodeHaas @carolemorrissey Yes i agree....   ...\n",
       "103335                                        @Wiley_Health\n",
       "103336                              Unbelievable @RiflePete\n",
       "103337    Thanks @TheJosephNaim for coming to see @Jerse...\n",
       "103338    @HarryTrevor8888 @blondebonnie94 It is incompa...\n",
       "103339    The day before the budget, Sportsbet had odds ...\n",
       "103340    @sophie_walsh9 Where was this Sophie ? He was ...\n",
       "103341    People who are not scientists but who question...\n",
       "103342    My mom came up to visit this morning and to ru...\n",
       "103343             TS7 \\u2764\\ufe0f https://t.co/OQOaAft5Fg\n",
       "103344    @iamtheoracle I know, I know. Those people are...\n",
       "103345    @StephanieLoveUK Already seen enough there\\u20...\n",
       "103346                  @colinhussey22 gorillaz\\ud83d\\udc4c\n",
       "103347    @insufferablscot @tinyelbows_ Part of me think...\n",
       "103348                            @CDeLaFuentez Lol exactly\n",
       "103349    @rmharmon2004 outside of the context ofwrestli...\n",
       "103350                    @KirstyWebeck So SMASH it Kirsty!\n",
       "103351    \\u0e04\\u0e34\\u0e14\\u0e16\\u0e36\\u0e07\\u0e2b\\u0e...\n",
       "103352    @tbdalton5 That they are! I have had 2 in my l...\n",
       "103353    @GrayConnolly @GemmaTognini @ElizaJBarr You\\u2...\n",
       "103354    @australian @IzzyFolau very interesting that y...\n",
       "103355                    Solzinho bom pra ir na praia hoje\n",
       "103356    @suzdavies13 Of me??? Thank you, surely you do...\n",
       "103357    @GraceSpelman have you heard of Dune? seen som...\n",
       "103358        Id bang you harder than an old Chevrolet door\n",
       "103359    @Mezmfield @chief_comanche @Utopiana @fraser_a...\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdir = path.join(getcwd(), \"2019S1-proj2-datah\")\n",
    "\n",
    "train_data = \"train-raw.tsv\"\n",
    "test_data = \"test-raw.tsv\"\n",
    "dev_data = \"dev-raw.tsv\"\n",
    "\n",
    "train_fpath = path.join(fdir, train_data)\n",
    "\n",
    "train = pd.read_csv(train_fpath, sep=\"\\t\")\n",
    "classes = train['Location'].unique()\n",
    "\n",
    "data.tail(50)['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GeoTagger:\n",
    "    def __init__(self, classes, classifier=\"MultinomialNB\", feature_selection=\"IGR\"):\n",
    "        self.classes = classes\n",
    "        self.exclude = set()\n",
    "        self.class_feature_sets = {label: defaultdict() for label in class}\n",
    "        self.classifier = classifier # MultinomialNB, SVM\n",
    "        self.stopwords = set(stopwords.words('english'))\n",
    "        self.\n",
    "        \n",
    "    def train(self, x, y):\n",
    "        pass\n",
    "    \n",
    "    def test(self, x, y):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x):\n",
    "        pass\n",
    "    \n",
    "    def evaluate(self, x, y):\n",
    "        pass\n",
    "    \n",
    "    def preprocess(self, x, y):\n",
    "        \"\"\"\n",
    "         - Filter rare words (urls, typos rare names, punctuation symbols)\n",
    "         - calculate word frequencies \n",
    "         - metadata\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def filter(self, x):\n",
    "        \"\"\"\n",
    "        https://towardsdatascience.com/extracting-twitter-data-pre-processing-and-sentiment-analysis-using-python-3-0-7192bd8b47cf\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def feature_weight(self, x):\n",
    "        \"\"\"\n",
    "        calculate frequencies of data \n",
    "        Measure frequency and divide by sum of freqencies of all words\n",
    "        \"\"\"\n",
    "    def feature_selection(self, x):\n",
    "        \"\"\"\n",
    "        (1) Information Gain Ratio (IGR) - across all states S, is \n",
    "            defined as the ratio between its information gain value IG, \n",
    "            which measures the decrease in class entropy H that w brings,\n",
    "            and its intrinsic entropy IV, which measures the entropy of \n",
    "            the presence versus the absence of that word\n",
    "            \n",
    "        (2) Word Locality Heuristic (WLH) - promotes words primarily \n",
    "            associated with one location. measure the probability of \n",
    "            a word occurring in a state, divided by its probability to \n",
    "            appear in any state. Then, for a given word w, we define the \n",
    "            WLH as the maximum such probability across all the states S\n",
    "        \"\"\"\n",
    "    def filter_tweet(self, tweet):\n",
    "        word_tokens = word_tokenize(tweet)\n",
    "        \n",
    "        filtered = [filter_word(w) for w in word_tokens if w not in self.stopwords]\n",
    "        \n",
    "        return ' '.join(filtered)\n",
    "        \n",
    "    def filter_word(self, word):\n",
    "        # extract keywords from hashtag \n",
    "        if self._is_hashtag(w):\n",
    "            w = self.process_hashtag(w)\n",
    "        # potentially cross-reference individuals mentioned? or discard\n",
    "        elif self._is_mention(w):\n",
    "            w = self.process_mention(w)\n",
    "        # remove ascii characters \n",
    "        else:\n",
    "            re.sub(r'[^\\x00-\\x7f]', r' ', w)\n",
    "        return w\n",
    "                \n",
    "    def _is_hashtag(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"#\"\n",
    "    \n",
    "    def _process_hashtag(self, x):\n",
    "        pass\n",
    "    \n",
    "    def _is_mention(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"@\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
