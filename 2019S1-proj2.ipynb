{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Build Baseline Location Classifier\\nApproaches:\\n- instance representation in form of 'bag' of words \\n    - features: word frequencies, metadata \\n    - exclude rare words in data set (used by less than 3 users )\\n- gramatical structure with NLP\\n- model instances in terms of authors instead of documents \\n\\n\\n- Baseline Classifier: Naive Bayes Model ()\\n\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Build Baseline Location Classifier\n",
    "Approaches:\n",
    "- instance representation in form of 'bag' of words \n",
    "    - features: word frequencies, metadata \n",
    "    - exclude rare words in data set (used by less than 3 users )\n",
    "- gramatical structure with NLP\n",
    "- model instances in terms of authors instead of documents \n",
    "\n",
    "\n",
    "- Baseline Classifier: Naive Bayes Model ()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from os import path, getcwd\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re, string\n",
    "import validators\n",
    "from info_gain.info_gain import info_gain_ratio\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# nltk.download('english')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\farha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import data into pandas format from original csv/tsv file formats for \n",
    "the top 10/50/100 train/test/dev data \n",
    "\"\"\"\n",
    "\n",
    "fdir = path.join(getcwd(), \"2019S1-proj2-datah\")\n",
    "\n",
    "# train_data = \"train-top10.csv\"\n",
    "# test_data = \"test-top10.csv\"\n",
    "# dev_data = \"dev-top10.csv\"\n",
    "\n",
    "train_data = \"train-raw.tsv\"\n",
    "test_data = \"test-raw.tsv\"\n",
    "dev_data = \"dev-raw.tsv\"\n",
    "\n",
    "train_fpath = path.join(fdir, train_data)\n",
    "test_fpath = path.join(fdir, test_data)\n",
    "dev_fpath = path.join(fdir, dev_data)\n",
    "\n",
    "train = pd.read_csv(train_fpath, sep=\"\\t\", index_col=\"Instance_ID\", encoding=\"utf_8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = ['Text']\n",
    "output = 'Location'\n",
    "x_train = train[inputs]\n",
    "y_train = train[output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTagger:\n",
    "    _FEATURE_SELECTION = [\"info_gain_ratio\", \"word_locality_heuristic\", 'tf_idf']\n",
    "    _VOTING_STRATEGY = [\"simple_voting\", \"bagging\", \"stacking\", \"random_forest\", \"boosting\"]\n",
    "    _CLASSIFIERS = [\"Zero-R\", \"One-R\", \"Decision-Tree\", \"MultinomialNB\", \"SVM\", \"SemiSupervised\", \"KNN\"]\n",
    "    _EVALUATION_METRIC = [\"accuracy\", \"precision_recall_f-score\"]\n",
    "#     _EVALUATION METHOD = [\"Cross-Validation\"]\n",
    "                          \n",
    "    def __init__(self, inputs, target, classifier_set=[\"MultinomialNB\"], voting_strategy=\"simple_voting\", feature_selection=\"tf_idf\", seed=500):\n",
    "        self.inputs = inputs\n",
    "        self.target = target\n",
    "        self.exclude = set()\n",
    "        self.classifier_set = classifier_set\n",
    "        self.voting_strategy = voting_strategy\n",
    "        self.feature_selection = feature_selection\n",
    "        self.classifier_set = self._combine_classifier_set(classifier_set)\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        self.vectorizer = TfidfVectorizer(stop_words=set(stopwords.words('english')))\n",
    "        np.random.seed(seed)\n",
    "                \n",
    "    def _combine_classifier_set(self, classifiers):\n",
    "        classifier_set = defaultdict()\n",
    "        \n",
    "        for classifier in classifiers:\n",
    "            if not classifier in GeoTagger._CLASSIFIERS:\n",
    "                print(\"Invalid Classifier: {}. Choose one of \\\n",
    "                ({})\".format(classifier, \", \".join(GeoTagger._CLASSIFIERS)))\n",
    "                continue\n",
    "                \n",
    "            if GeoTagger._CLASSIFIERS.index(classifier) == 0:\n",
    "                classifier_set[classifier] = DummyClassifier(strategy='most_frequent')\n",
    "            elif GeoTagger._CLASSIFIERS.index(classifier) == 1:\n",
    "                classifier_set[classifier] = DecisionTreeClassifier(max_depth=1, criterion=\"entropy\")\n",
    "            elif GeoTagger._CLASSIFIERS.index(classifier)==2:\n",
    "                classifier_set[classifier] = DecisionTreeClassifier(max_depth=None, criterion=\"entropy\")\n",
    "            elif GeoTagger._CLASSIFIERS.index(classifier)==3:\n",
    "                classifier_set[classifier] = MultinomialNB()\n",
    "        return classifier_set\n",
    "        \n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        trains a classifier given the training data and their corresponding class labels\n",
    "        \"\"\"\n",
    "        self.classes = y.unique()\n",
    "        x = self.preprocess(X, y, train=True)\n",
    "        \n",
    "        for classifier in self.classifier_set.values():\n",
    "            classifier.fit(X, y)\n",
    "          \n",
    "    def predict(self, X):\n",
    "        x = self.preprocess(X)\n",
    "        predictions = pd.DataFrame()\n",
    "                \n",
    "        for name, classifier in self.classifier_set.items():\n",
    "            classifier_prediction = classifier.predict(x, y)\n",
    "            predictions[name] = classifier_predictions\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, ybar, y, metric):\n",
    "        #TODO: eval method\n",
    "        if not metric in GeoTagger._EVALUATION_METRIC:\n",
    "                print(\"Invalid Evaluation Metric: {}. Choose one of \\\n",
    "                ({})\".format(metric, \", \".join(GeoTagger._EVALUATION_METRIC))) \n",
    "                return\n",
    "            \n",
    "        if metric == \"accuracy\":\n",
    "            score = accuracy_score(ybar, y)\n",
    "        if metric == \"precision_recall_f-score_with_macro\":\n",
    "            score = accuracy_score(ybar, y)\n",
    "        if metric == \"recall\":\n",
    "            score = accuracy_score(ybar, y)            \n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def preprocess(self, x, y=None, train=False):\n",
    "        \"\"\"\n",
    "         - Filter rare words (urls, typos rare names, punctuation symbols)\n",
    "         - calculate word frequencies \n",
    "         - metadata\n",
    "        \"\"\"\n",
    "        if train:\n",
    "            x = self.filter(x)\n",
    "            x = self.feature_selection(x, y)\n",
    "        else:\n",
    "            features = self.vectoriser.transform(x[self.inputs])\n",
    "        return x     \n",
    "        \n",
    "    def feature_selection(self, x):\n",
    "        \"\"\"\n",
    "        (1) Information Gain Ratio (IGR) - across all states S, is \n",
    "            defined as the ratio between its information gain value IG, \n",
    "            which measures the decrease in class entropy H that w brings,\n",
    "            and its intrinsic entropy IV, which measures the entropy of \n",
    "            the presence versus the absence of that word\n",
    "            \n",
    "        (2) Word Locality Heuristic (WLH) - promotes words primarily \n",
    "            associated with one location. measure the probability of \n",
    "            a word occurring in a state, divided by its probability to \n",
    "            appear in any state. Then, for a given word w, we define the \n",
    "            WLH as the maximum such probability across all the states S\n",
    "        \"\"\"\n",
    "        if self.feature_selection == \"IGR\":\n",
    "#             return information_gain_ratio(x)\n",
    "             return\n",
    "        elif self.feature_selection == \"WLH\":\n",
    "#             return word_locality_weight(x)\n",
    "            return\n",
    "        else:\n",
    "            print(\"Invalid Feature Selection method: {}. Choose one of \\\n",
    "            ({})\".format(self.feature_selection, \", \".join(GeoTagger._FEATURE_SELECTION)))\n",
    "        \n",
    "    def word_locality_weight(self, x):\n",
    "        \"\"\"\n",
    "        calculate frequencies of data \n",
    "        Measure frequency and divide by sum of freqencies of all words\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def information_gain_ratio(self, x):\n",
    "#         return info_gain_ratio\n",
    "        pass\n",
    "    \n",
    "    def tf_idf(self, x, y):\n",
    "        location_word_list = {label: '' for label in self.classes}\n",
    "\n",
    "        print(x.index[-1], y.index[-1])\n",
    "        for index in x.index:\n",
    "            location_word_list[y.iloc[index]] += x.iloc[index] + \" \"\n",
    "        print(location_word_list.values())\n",
    "        features = self.vectorizer.fit_transform(location_word_list.values)\n",
    "        return x\n",
    "        \n",
    "    def filter(self, text):\n",
    "        return ' '.join(self.filter_word(w) for w in text.split())\n",
    "        \n",
    "    def filter_word(self, word):\n",
    "        word = word.lower()\n",
    "        # extract keywords from hashtag \n",
    "        if self._is_hyperlink(word):\n",
    "            return ''\n",
    "        elif self._is_hashtag(word):\n",
    "            word = self._process_hashtag(word)\n",
    "        # potentially cross-reference individuals mentioned? or discard\n",
    "        elif self._is_mention(word):\n",
    "            word = self._process_mention(word)\n",
    "        # remove ascii characters \n",
    "        else:\n",
    "            word = self._ascii_to_unicode(word)\n",
    "            word = self._word_stem(word)\n",
    "            # .decode(\"unicode_escape\").encode('utf-8')\n",
    "            word = re.sub(r'[^\\w\\s]',' ', word)\n",
    "#         print(word)\n",
    "        return word\n",
    "                \n",
    "    def _is_hashtag(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"#\"\n",
    "    \n",
    "    def _is_mention(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"@\"\n",
    "    \n",
    "    def _is_hyperlink(self, word):\n",
    "        return validators.url(word)\n",
    "    \n",
    "    def _process_hashtag(self, word):\n",
    "        return word[1:]\n",
    "    \n",
    "    def _process_mention(self, word):\n",
    "        return word[1:]\n",
    "    \n",
    "    def _ascii_to_unicode(self, word):\n",
    "        for uescape in re.findall(r'(\\\\u[0-9a-z]{4})', word):\n",
    "            try:\n",
    "#                 print(uescape.encode('utf-8').decode('unicode-escape'), type(uescape.encode('utf-8').decode('unicode-escape')))\n",
    "#                 word = re.sub(uescape, uescape.encode('utf-8'), word)\n",
    "#                 print(word)\n",
    "#                 print(uescape, type(uescape))\n",
    "                word = re.sub(uescape, '', word)  \n",
    "            except (UnicodeDecodeError, Exception):\n",
    "                print(\"Failed to decode: {}\".format(uescape))\n",
    "        return word\n",
    "    \n",
    "    def _word_stem(self, word):\n",
    "        return self.stemmer.stem(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Instance_ID</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\ud83c\\udf17 @ Melbourne, Victoria, Australia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@theage Of course it costs more, minimum stand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hope people make just as much noise as they di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pouring the perfect Prosecco \\ud83e\\udd42\\ud83...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$LNY losing traction at 0.014, see this retrac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\u0e44\\u0e21\\u0e48\\u0e44\\u0e2b\\u0e27\\u0e41\\u0e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>@ashleighjayy_ Me @ this bitch https://t.co/8J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@AnaOLFan I \\u2764\\ufe0f you - I could never b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>@ihatejoelkim Welcome to Australia! Hoping you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mea evolve conference @markbouris session. I L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>First taste of Winter in Autumn https://t.co/0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\\u0643\\u064a\\u0641 \\u064a\\u0634\\u0648\\u0641 \\u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>#SelfieSunday #Lost #Tabby &amp;amp; White/Caramel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>@artsdesire Give her credit, she looks incredi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>@Pringster78 @Chadderbox2018 I kind of do the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@gramercypark It\\u2019s great isn\\u2019t it?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://t.co/RT37Cj5KRF  I date her, not MJ xo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>@AussieAndyCx @ImScaredq Deadest Kane would de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>if i see got spoilers (and esp not tagged or a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>\\u0639\\u064e\\u0644\\u064a\\u0643 \\u0639\\u0648\\u0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>@pleaseuseaussie @RodS108443078 Hope so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>x2 babe \\ud83d\\ude2b biggest scam I tell you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>@idgimaddy @EXOGlobal @B_hundred_Hyun @1992050...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sbren sbeve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Grassy, bitter (Kiwi) hops, clean - Drinking a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@FatherBob  Father Bob,  Thankyou I went to an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>30th Birthday Dinner \\ud83e\\udd42\\ud83c\\udf88\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>@clementine_ford Us fellas were all drawn to i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>https://t.co/nejALF6216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>KOKO BUNNY! The Easter Bunny arrived at my doo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103335</th>\n",
       "      <td>Wow, I\\u2019m not very good at getting the pic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103336</th>\n",
       "      <td>I gather the Govt wants to further clog up Pun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103337</th>\n",
       "      <td>Visualization is the key to success. Dream big...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103338</th>\n",
       "      <td>Countries I\\u2019ve Been To:  Fiji \\ud83c\\udde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103339</th>\n",
       "      <td>@TaodeHaas @carolemorrissey Yes i agree....   ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103340</th>\n",
       "      <td>@Wiley_Health</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103341</th>\n",
       "      <td>Unbelievable @RiflePete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103342</th>\n",
       "      <td>Thanks @TheJosephNaim for coming to see @Jerse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103343</th>\n",
       "      <td>@HarryTrevor8888 @blondebonnie94 It is incompa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103344</th>\n",
       "      <td>The day before the budget, Sportsbet had odds ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103345</th>\n",
       "      <td>@sophie_walsh9 Where was this Sophie ? He was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103346</th>\n",
       "      <td>People who are not scientists but who question...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103347</th>\n",
       "      <td>My mom came up to visit this morning and to ru...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103348</th>\n",
       "      <td>TS7 \\u2764\\ufe0f https://t.co/OQOaAft5Fg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103349</th>\n",
       "      <td>@iamtheoracle I know, I know. Those people are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103350</th>\n",
       "      <td>@StephanieLoveUK Already seen enough there\\u20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103351</th>\n",
       "      <td>@colinhussey22 gorillaz\\ud83d\\udc4c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103352</th>\n",
       "      <td>@insufferablscot @tinyelbows_ Part of me think...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103353</th>\n",
       "      <td>@CDeLaFuentez Lol exactly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103354</th>\n",
       "      <td>@rmharmon2004 outside of the context ofwrestli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103355</th>\n",
       "      <td>@KirstyWebeck So SMASH it Kirsty!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103356</th>\n",
       "      <td>\\u0e04\\u0e34\\u0e14\\u0e16\\u0e36\\u0e07\\u0e2b\\u0e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103357</th>\n",
       "      <td>@tbdalton5 That they are! I have had 2 in my l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103358</th>\n",
       "      <td>@GrayConnolly @GemmaTognini @ElizaJBarr You\\u2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103359</th>\n",
       "      <td>@australian @IzzyFolau very interesting that y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103360</th>\n",
       "      <td>Solzinho bom pra ir na praia hoje</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103361</th>\n",
       "      <td>@suzdavies13 Of me??? Thank you, surely you do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103362</th>\n",
       "      <td>@GraceSpelman have you heard of Dune? seen som...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103363</th>\n",
       "      <td>Id bang you harder than an old Chevrolet door</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103364</th>\n",
       "      <td>@Mezmfield @chief_comanche @Utopiana @fraser_a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103360 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          Text\n",
       "Instance_ID                                                   \n",
       "1            \\ud83c\\udf17 @ Melbourne, Victoria, Australia ...\n",
       "2            @theage Of course it costs more, minimum stand...\n",
       "3            Hope people make just as much noise as they di...\n",
       "4            Pouring the perfect Prosecco \\ud83e\\udd42\\ud83...\n",
       "5            $LNY losing traction at 0.014, see this retrac...\n",
       "6            \\u0e44\\u0e21\\u0e48\\u0e44\\u0e2b\\u0e27\\u0e41\\u0e...\n",
       "7            @ashleighjayy_ Me @ this bitch https://t.co/8J...\n",
       "8            @AnaOLFan I \\u2764\\ufe0f you - I could never b...\n",
       "9            @ihatejoelkim Welcome to Australia! Hoping you...\n",
       "10           Mea evolve conference @markbouris session. I L...\n",
       "11           First taste of Winter in Autumn https://t.co/0...\n",
       "12           \\u0643\\u064a\\u0641 \\u064a\\u0634\\u0648\\u0641 \\u...\n",
       "13           #SelfieSunday #Lost #Tabby &amp; White/Caramel...\n",
       "14           @artsdesire Give her credit, she looks incredi...\n",
       "15           @Pringster78 @Chadderbox2018 I kind of do the ...\n",
       "16                @gramercypark It\\u2019s great isn\\u2019t it?\n",
       "17           https://t.co/RT37Cj5KRF  I date her, not MJ xo...\n",
       "18           @AussieAndyCx @ImScaredq Deadest Kane would de...\n",
       "19           if i see got spoilers (and esp not tagged or a...\n",
       "20           \\u0639\\u064e\\u0644\\u064a\\u0643 \\u0639\\u0648\\u0...\n",
       "21                    @pleaseuseaussie @RodS108443078 Hope so.\n",
       "22               x2 babe \\ud83d\\ude2b biggest scam I tell you.\n",
       "23           @idgimaddy @EXOGlobal @B_hundred_Hyun @1992050...\n",
       "24                                                 sbren sbeve\n",
       "25           Grassy, bitter (Kiwi) hops, clean - Drinking a...\n",
       "26           @FatherBob  Father Bob,  Thankyou I went to an...\n",
       "27           30th Birthday Dinner \\ud83e\\udd42\\ud83c\\udf88\\...\n",
       "28           @clementine_ford Us fellas were all drawn to i...\n",
       "29                                     https://t.co/nejALF6216\n",
       "30           KOKO BUNNY! The Easter Bunny arrived at my doo...\n",
       "...                                                        ...\n",
       "103335       Wow, I\\u2019m not very good at getting the pic...\n",
       "103336       I gather the Govt wants to further clog up Pun...\n",
       "103337       Visualization is the key to success. Dream big...\n",
       "103338       Countries I\\u2019ve Been To:  Fiji \\ud83c\\udde...\n",
       "103339       @TaodeHaas @carolemorrissey Yes i agree....   ...\n",
       "103340                                           @Wiley_Health\n",
       "103341                                 Unbelievable @RiflePete\n",
       "103342       Thanks @TheJosephNaim for coming to see @Jerse...\n",
       "103343       @HarryTrevor8888 @blondebonnie94 It is incompa...\n",
       "103344       The day before the budget, Sportsbet had odds ...\n",
       "103345       @sophie_walsh9 Where was this Sophie ? He was ...\n",
       "103346       People who are not scientists but who question...\n",
       "103347       My mom came up to visit this morning and to ru...\n",
       "103348                TS7 \\u2764\\ufe0f https://t.co/OQOaAft5Fg\n",
       "103349       @iamtheoracle I know, I know. Those people are...\n",
       "103350       @StephanieLoveUK Already seen enough there\\u20...\n",
       "103351                     @colinhussey22 gorillaz\\ud83d\\udc4c\n",
       "103352       @insufferablscot @tinyelbows_ Part of me think...\n",
       "103353                               @CDeLaFuentez Lol exactly\n",
       "103354       @rmharmon2004 outside of the context ofwrestli...\n",
       "103355                       @KirstyWebeck So SMASH it Kirsty!\n",
       "103356       \\u0e04\\u0e34\\u0e14\\u0e16\\u0e36\\u0e07\\u0e2b\\u0e...\n",
       "103357       @tbdalton5 That they are! I have had 2 in my l...\n",
       "103358       @GrayConnolly @GemmaTognini @ElizaJBarr You\\u2...\n",
       "103359       @australian @IzzyFolau very interesting that y...\n",
       "103360                       Solzinho bom pra ir na praia hoje\n",
       "103361       @suzdavies13 Of me??? Thank you, surely you do...\n",
       "103362       @GraceSpelman have you heard of Dune? seen som...\n",
       "103363           Id bang you harder than an old Chevrolet door\n",
       "103364       @Mezmfield @chief_comanche @Utopiana @fraser_a...\n",
       "\n",
       "[103360 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-dad3b239de00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m )\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mx_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-f8ce64fc5e2c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \"\"\"\n\u001b[0;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier_set\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-f8ce64fc5e2c>\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(self, x, y, train)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \"\"\"\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_selection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-f8ce64fc5e2c>\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfilter_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   3612\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3613\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3614\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3616\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "gt = GeoTagger(inputs[0], output)\n",
    "\n",
    "gt = GeoTagger(\n",
    "    inputs = inputs[0],\n",
    "    target = output,\n",
    "    classifier_set = [\"MultinomialNB\"], \n",
    "    voting_strategy = \"simple_voting\", \n",
    "    seed=500\n",
    ")\n",
    "\n",
    "x_features = gt.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the class labels of a set of test data\n",
    "# ybar = self.predict(clf, x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate classifier performance\n",
    "# score = self.evaluate(ybar, dev_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Resources\n",
    "- https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "print(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
