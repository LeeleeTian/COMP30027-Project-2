{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Build Baseline Location Classifier\\nApproaches:\\n- instance representation in form of 'bag' of words \\n    - features: word frequencies, metadata \\n    - exclude rare words in data set (used by less than 3 users )\\n- gramatical structure with NLP\\n- model instances in terms of authors instead of documents \\n\\n\\n- Baseline Classifier: Naive Bayes Model ()\\n\\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Build Baseline Location Classifier\n",
    "Approaches:\n",
    "- instance representation in form of 'bag' of words \n",
    "    - features: word frequencies, metadata \n",
    "    - exclude rare words in data set (used by less than 3 users )\n",
    "- gramatical structure with NLP\n",
    "- model instances in terms of authors instead of documents \n",
    "\n",
    "\n",
    "- Baseline Classifier: Naive Bayes Model ()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from os import path, getcwd\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import re, string\n",
    "import validators\n",
    "from info_gain.info_gain import info_gain_ratio\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import chi2\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# nltk.download('english')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdir = path.join(getcwd(), \"2019S1-proj2-datah\")\n",
    "\n",
    "train_data = \"train-raw.tsv\"\n",
    "test_data = \"test-raw.tsv\"\n",
    "dev_data = \"dev-raw.tsv\"\n",
    "\n",
    "train_fpath = path.join(fdir, train_data)\n",
    "test_fpath = path.join(fdir, test_data)\n",
    "dev_fpath = path.join(fdir, dev_data)\n",
    "\n",
    "train = pd.read_csv(train_fpath, encoding=\"utf_8\", delimiter=\"\\t\", index_col=\"Instance_ID\")\n",
    "test = pd.read_csv(test_fpath, encoding=\"utf_8\", delimiter=\"\\t\" , index_col=\"Instance_ID\")\n",
    "dev = pd.read_csv(dev_fpath, encoding=\"utf_8\", delimiter=\"\\t\" , index_col=\"Instance_ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = 'Text'\n",
    "output = 'Location'\n",
    "x_train = train[inputs]\n",
    "y_train = train[output]\n",
    "x_test = test[inputs]\n",
    "y_test = test[output]\n",
    "x_dev = dev[inputs]\n",
    "y_dev = dev[output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeoTagger:\n",
    "    _FEATURE_SELECTION = [\"baseline_10\", \"baseline_50\", \"baseline_100\", \"info_gain_ratio\", \"word_locality_heuristic\", \"tf_idf\"]\n",
    "    _VOTING_STRATEGY = [\"simple_voting\", \"bagging\", \"stacking\", \"random_forest\", \"boosting\"]\n",
    "    _CLASSIFIERS = [\"Zero-R\", \"One-R\", \"Decision-Tree\", \"MultinomialNB\", \"LinearSVM\", \"SemiSupervised\"]\n",
    "    _EVALUATION_METRIC = [\"accuracy\", \"precision_recall_f-score_with_macro\", \"precision_recall_f-score_with_micro\"]\n",
    "    \n",
    "    def __init__(self, inputs, target, classifier_set=[\"MultinomialNB\"], voting_strategy=\"simple_voting\", feature_selection_method=\"baseline_100\", seed=500, combine_classifiers=False):\n",
    "        self.inputs = inputs\n",
    "        self.target = target\n",
    "        self.classifier_set = classifier_set\n",
    "        self.voting_strategy = voting_strategy\n",
    "        self.feature_selection_method = feature_selection_method\n",
    "        self.seed = seed\n",
    "        np.random.seed(seed)\n",
    "        self.stemmer = SnowballStemmer('english')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.combine_classifiers = combine_classifiers\n",
    "        self.classifier_set = self._combine_classifier_set(classifier_set)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        \"\"\"\n",
    "        trains a classifier given the training data and their corresponding class labels\n",
    "        \"\"\"\n",
    "        self.classes = y.unique()\n",
    "        X = self.preprocess(X, y, train=True)\n",
    "        \n",
    "        for classifier in self.classifier_set.values():\n",
    "            print(type(classifier))\n",
    "            classifier.fit(X, y)\n",
    "          \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        predicts a set of classifiers given some development data\n",
    "        \"\"\"\n",
    "        X = self.preprocess(X)\n",
    "        predictions = pd.DataFrame()\n",
    "                \n",
    "        for name, classifier in self.classifier_set.items():\n",
    "            classifier_prediction = classifier.predict(X)\n",
    "            predictions[name] = classifier_prediction\n",
    "            \n",
    "        return predictions\n",
    "    \n",
    "    def evaluate(self, ybar, y, metric):\n",
    "        \"\"\"\n",
    "        evaluates a class' predictions given the correct class labels and an evaluation metric\n",
    "        \"\"\"\n",
    "        if not metric in GeoTagger._EVALUATION_METRIC:\n",
    "                print(\"Invalid Evaluation Metric: {}. Choose one of \\\n",
    "                ({})\".format(metric, \", \".join(GeoTagger._EVALUATION_METRIC))) \n",
    "                return\n",
    "            \n",
    "        score_set = defaultdict()\n",
    "        classifiers = list(ybar)\n",
    "        for classifier in classifiers:\n",
    "            if metric == \"accuracy\":\n",
    "                score_set[classifier] = accuracy_score(ybar[classifier], y)\n",
    "            if metric == \"precision_recall_f-score_with_macro\":\n",
    "                score_set[classifier] = precision_recall_fscore_support(y, ybar[classifier], average='macro')\n",
    "            if metric == \"precision_recall_f-score_with_micro\":\n",
    "                score_set[classifier] = precision_recall_fscore_support(y, ybar[classifier], average='micro')\n",
    "\n",
    "        return score_set\n",
    "    \n",
    "    def preprocess(self, X, y=None, train=False):\n",
    "        \"\"\"\n",
    "         - Filter rare words (urls, typos rare names, punctuation symbols)\n",
    "         - calculate word frequencies \n",
    "         - metadata\n",
    "        \"\"\"\n",
    "        X = self.filter(X)\n",
    "\n",
    "        if train:\n",
    "            self.feature_selection(X, y)\n",
    "        \n",
    "        X = self.bag_of_words(X)\n",
    "            \n",
    "        return X \n",
    "    \n",
    "    def bag_of_words(self, X):\n",
    "        _x = pd.DataFrame(\n",
    "            [[(word in text) for word in sorted(list(self.features))] for text in X.values],\n",
    "            index=X.index, \n",
    "            columns=self.features,\n",
    "            dtype=np.uint8\n",
    "        )\n",
    "        #print(_x)\n",
    "        \n",
    "        return _x\n",
    "    \n",
    "    def _combine_classifier_set(self, classifiers):\n",
    "        classifier_set = defaultdict()\n",
    "        \n",
    "        for classifier in classifiers:\n",
    "            if not classifier in GeoTagger._CLASSIFIERS:\n",
    "                print(\"Invalid Classifier: {}. Choose one of \\\n",
    "                ({})\".format(classifier, \", \".join(GeoTagger._CLASSIFIERS)))\n",
    "                continue\n",
    "                \n",
    "            if classifier == \"Zero-R\":\n",
    "                classifier_set[classifier] = DummyClassifier(strategy='most_frequent', random_state=self.seed)\n",
    "            elif classifier == \"One-R\":\n",
    "                classifier_set[classifier] = DecisionTreeClassifier(max_depth=1, criterion=\"entropy\", random_state=self.seed)\n",
    "            elif classifier == \"Decision-Tree\":\n",
    "                classifier_set[classifier] = DecisionTreeClassifier(max_depth=None, criterion=\"entropy\", random_state=self.seed)\n",
    "            elif classifier == \"MultinomialNB\":\n",
    "                classifier_set[classifier] = MultinomialNB()\n",
    "            elif classifier == \"LinearSVM\":\n",
    "                continue\n",
    "            elif classifier == \"SemiSupervised\":\n",
    "                continue\n",
    "        \n",
    "        if self.combine_classifiers:\n",
    "            #TODO: combine classifiers\n",
    "            pass\n",
    "        else: \n",
    "            return classifier_set\n",
    "                \n",
    "    def feature_selection(self, X, y):\n",
    "        \"\"\"\n",
    "        (1) Information Gain Ratio (IGR) - across all states S, is \n",
    "            defined as the ratio between its information gain value IG, \n",
    "            which measures the decrease in class entropy H that w brings,\n",
    "            and its intrinsic entropy IV, which measures the entropy of \n",
    "            the presence versus the absence of that word\n",
    "            \n",
    "        (2) Word Locality Heuristic (WLH) - promotes words primarily \n",
    "            associated with one location. measure the probability of \n",
    "            a word occurring in a state, divided by its probability to \n",
    "            appear in any state. Then, for a given word w, we define the \n",
    "            WLH as the maximum such probability across all the states S\n",
    "        \"\"\"\n",
    "        if self.feature_selection_method not in GeoTagger._FEATURE_SELECTION:\n",
    "            print(\"Invalid Feature Selection method: {}. Choose one of \\\n",
    "            ({})\".format(self.feature_selection_method, \", \".join(GeoTagger._FEATURE_SELECTION)))\n",
    "            return \n",
    "        \n",
    "        if self.feature_selection_method == \"baseline_10\":\n",
    "            self.baseline_heuristic(X, y, \"10\")\n",
    "        elif self.feature_selection_method == \"baseline_50\":\n",
    "            self.baseline_heuristic(X, y, \"50\")\n",
    "        elif self.feature_selection_method == \"baseline_100\":\n",
    "            self.baseline_heuristic(X, y, \"100\")\n",
    "        elif self.feature_selection_method == \"info_gain_ratio\":\n",
    "#             self.information_gain_ratio(x)\n",
    "            return\n",
    "        elif self.feature_selection_method == \"word_locality_heuristic\":\n",
    "            self.word_locality_weight(X, y)\n",
    "        elif self.feature_selection_method == \"tf_idf\":\n",
    "            self.tf_idf(X, y)\n",
    "\n",
    "    def baseline_heuristic(self, X, y, top_n):\n",
    "#         fdir = path.join(getcwd(), \"2019S1-proj2-datah\")\n",
    "        feature_fpath = path.join(fdir, \"train-top\" + top_n + \".csv\")\n",
    "        \n",
    "        if not path.exists(feature_fpath):\n",
    "            print(\"Baseline Heuristic path {} does not exist\".format(feature_fpath))\n",
    "            return\n",
    "        \n",
    "        features = open(feature_fpath).readline()\n",
    "        features = features.split(\",\")\n",
    "        features.remove(\"Instance_ID\")\n",
    "        features.remove(\"Location\\n\")\n",
    "        self.features = set(features)\n",
    "\n",
    "    def word_locality_weight(self, X, y):\n",
    "        \"\"\"\n",
    "        calculate frequencies of data \n",
    "        Measure frequency and divide by sum of freqencies of all words\n",
    "        \"\"\"\n",
    "        locations = self.classes + ['Total',]\n",
    "        word_location_weight = {label: defaultdict() for label in locations}\n",
    "        \n",
    "        for x_i, y_i in zip(X.index, y.index):\n",
    "            text = X.loc[x_i].split()\n",
    "            for word in text:\n",
    "                word_location_weight[y.loc[y_i]][word] += 1\n",
    "                word_location_weight[y.loc[y_i]]['Total'] += 1\n",
    "                word_location_weight['Total'][word] += 1\n",
    "                word_location_weight['Total']['Total'] += 1\n",
    "                \n",
    "    \n",
    "    def information_gain_ratio(self, x):\n",
    "#         return info_gain_ratio\n",
    "        pass\n",
    "    \n",
    "    def tf_idf(self, X, y):\n",
    "        vectorizer = TfidfVectorizer(stop_words=self.stop_words, max_features=400)\n",
    "\n",
    "        location_word_list = {label: '' for label in self.classes}\n",
    "\n",
    "        for x_i, y_i in zip(X.index, y.index):\n",
    "            location_word_list[y.loc[y_i]] += X.loc[x_i] + \" \"\n",
    "        \n",
    "        labels = location_word_list.keys()\n",
    "        corpus = location_word_list.values()\n",
    "        vectorizer.fit(corpus, labels)\n",
    "        self.features = set(vectorizer.get_feature_names())\n",
    "        \n",
    "    def filter(self, x):\n",
    "        return x.apply(self.filter_text)\n",
    "    \n",
    "    def filter_text(self, text):\n",
    "        return ' '.join(self.filter_word(w) for w in text.split())\n",
    "        \n",
    "    def filter_word(self, word):\n",
    "        word = word.lower()\n",
    "        # extract keywords from hashtag \n",
    "        if self._is_hyperlink(word):\n",
    "            return ''\n",
    "        elif self._is_hashtag(word):\n",
    "            word = self._process_hashtag(word)\n",
    "        # potentially cross-reference individuals mentioned? or discard\n",
    "        elif self._is_mention(word):\n",
    "            word = self._process_mention(word)\n",
    "        # remove ascii characters \n",
    "        else:\n",
    "            word = self._ascii_to_unicode(word)\n",
    "            word = self._word_stem(word)\n",
    "            word = re.sub(r'[^\\w\\s]',' ', word)\n",
    "        return word\n",
    "                \n",
    "    def _is_hashtag(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"#\"\n",
    "    \n",
    "    def _is_mention(self, word):\n",
    "        if len(word) == 0:\n",
    "            return False\n",
    "        return word[0] == \"@\"\n",
    "    \n",
    "    def _is_hyperlink(self, word):\n",
    "        return validators.url(word)\n",
    "    \n",
    "    def _process_hashtag(self, word):\n",
    "        return word[1:]\n",
    "    \n",
    "    def _process_mention(self, word):\n",
    "        return word[1:]\n",
    "    \n",
    "    def _ascii_to_unicode(self, word):\n",
    "        for uescape in re.findall(r'(\\\\u[0-9a-f]{4})', word):\n",
    "            try:\n",
    "#                 print(uescape.encode('utf-8').decode('unicode-escape'), type(uescape.encode('utf-8').decode('unicode-escape')))\n",
    "#                 word = re.sub(uescape, uescape.encode('utf-8'), word)\n",
    "#                 print(word)\n",
    "#                 print(uescape, type(uescape))\n",
    "                word = word.replace(uescape, '')  \n",
    "            except (UnicodeDecodeError, Exception):\n",
    "                print(\"Failed to decode: {}\".format(uescape))\n",
    "        return word\n",
    "    \n",
    "    def _word_stem(self, word):\n",
    "        return self.stemmer.stem(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.dummy.DummyClassifier'>\n",
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "<class 'sklearn.tree.tree.DecisionTreeClassifier'>\n",
      "<class 'sklearn.naive_bayes.MultinomialNB'>\n"
     ]
    }
   ],
   "source": [
    "gt = GeoTagger(\n",
    "    inputs = inputs,\n",
    "    target = output,\n",
    "    classifier_set = [\"Zero-R\", \"One-R\", \"Decision-Tree\", \"MultinomialNB\"],\n",
    "    voting_strategy = \"simple_voting\",\n",
    "    feature_selection_method = \"baseline_100\",\n",
    "    seed = 500,\n",
    "    combine_classifiers = False\n",
    ")\n",
    "\n",
    "gt.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the class labels of a set of test data\n",
    "ybars = gt.predict(x_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate classifier performanceq\n",
    "accScores = gt.evaluate(ybars, y_dev, \"accuracy\")\n",
    "otherScores = gt.evaluate(ybars, y_dev, \"precision_recall_f-score_with_micro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'Decision-Tree': 0.30273458445040213,\n",
       "             'MultinomialNB': 0.31563002680965146,\n",
       "             'One-R': 0.2693029490616622,\n",
       "             'Zero-R': 0.24997319034852547})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'Decision-Tree': (0.30273458445040213,\n",
       "              0.30273458445040213,\n",
       "              0.30273458445040213,\n",
       "              None),\n",
       "             'MultinomialNB': (0.31563002680965146,\n",
       "              0.31563002680965146,\n",
       "              0.31563002680965146,\n",
       "              None),\n",
       "             'One-R': (0.2693029490616622,\n",
       "              0.2693029490616622,\n",
       "              0.2693029490616622,\n",
       "              None),\n",
       "             'Zero-R': (0.24997319034852547,\n",
       "              0.24997319034852547,\n",
       "              0.24997319034852547,\n",
       "              None)})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "otherScores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Resources\n",
    "- https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len([1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
